Context Engineering

- Owner
  - Aiden Zepp
- Purpose
  - Understand production-quality context-engineering techniques and best practices to develop custom prompts for applications, internal tooling, scripts, and content windows.
- Experts
  - Expert 1: Riley Goodside
    - Who:
      - Staff Prompt Engineer (ex-Scale AI)
      - Widely recognized pioneer of prompt engineering demos
    - Focus:
      - Prompt engineering techniques and methodologies
      - Adversarial prompting and model behavior exploration
      - Production reliability and debugging
    - Why Follow:
      - Shares cutting-edge prompt tricks and practical debugging tips crucial to production reliability
      - Pioneer in demonstrating prompt engineering capabilities
    - Where:
      - X/Twitter: https://twitter.com/goodside
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 2: Amanda Askell
    - Who:
      - Research Scientist at Anthropic
      - AI alignment and prompt-steering researcher
    - Focus:
      - Prompt design for safe and controllable LLMs
      - Constitutional AI development
      - AI safety and alignment through prompting
    - Why Follow:
      - Explains why certain prompts succeed or fail and how to guide Claude effectively
      - Deep expertise in AI safety and constitutional AI
    - Where:
      - X/Twitter: https://twitter.com/AmandaAskell
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 3: Zack Witten
    - Who:
      - Prompt Engineer at Anthropic
      - Co-author of Claude prompt tips
    - Focus:
      - Practical prompt patterns and implementations
      - Jailbreak mitigation strategies
      - Enterprise deployment best practices
    - Why Follow:
      - Publishes real-world prompt iterations and lessons learned from enterprise deployments
      - Practical expertise in production prompt engineering
    - Where:
      - X/Twitter: https://twitter.com/ZackWitten
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 4: David Hershey
    - Who:
      - Prompt Engineer at Anthropic
      - Speaker on prompt best practices
    - Focus:
      - Structured prompting techniques
      - Tag-based reasoning methodologies
      - Large-context prompt optimization
    - Why Follow:
      - Shares insights on using tags to improve Claude reasoning
      - Expertise in structured approach to prompt engineering
    - Where:
      - X/Twitter: https://twitter.com/DavidSHershey
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 5: Alex Albert
    - Who:
      - Head of Developer Relations at Anthropic
      - Host of "Inside Prompt Engineering" sessions
    - Focus:
      - Developer education and community building
      - Prompt strategy development
      - Security considerations in prompting
    - Why Follow:
      - Curates prompt tips from Anthropic engineers and publishes deep-dives
      - Bridge between technical research and practical application
    - Where:
      - X/Twitter: https://twitter.com/alexalbert__
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 6: Isa Fulford
    - Who:
      - OpenAI Advocate
      - Co-instructor of DeepLearning.AI prompt engineering course
    - Focus:
      - Applied prompt patterns and techniques
      - OpenAI API usage and optimization
      - Educational content creation
    - Why Follow:
      - Provides clear, concise techniques aligned with OpenAI best-practice documentation
      - Strong educational approach to prompt engineering
    - Where:
      - X/Twitter: https://twitter.com/isafoul
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 7: Andrew Ng
    - Who:
      - AI pioneer and educator
      - Founder of DeepLearning.AI and Coursera
    - Focus:
      - AI education and democratization
      - Prompt engineering pedagogy
      - Structured learning approaches
    - Why Follow:
      - Offers free prompt-engineering courses and distilled frameworks for practitioners
      - Brings academic rigor to practical prompt engineering education
    - Where:
      - LinkedIn: https://www.linkedin.com/in/andrewyng
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 8: Elvis Saravia
    - Who:
      - Founder of DAIR.AI
      - Curator of Prompt Engineering Guide
    - Focus:
      - Aggregating and organizing prompt research
      - Advanced prompting techniques development
      - Community knowledge sharing
    - Why Follow:
      - Maintains the most comprehensive, continually updated prompt-engineering wiki
      - Central hub for prompt engineering research and techniques
    - Where:
      - X/Twitter: https://twitter.com/omarsar0
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 9: Harrison Chase
    - Who:
      - CEO and Co-founder of LangChain
    - Focus:
      - Prompt templating and orchestration
      - Chaining prompts with tools and memory
      - Production LLM workflow development
    - Why Follow:
      - Shares practical recipes for building production LLM workflows
      - Pioneer in prompt chaining and orchestration frameworks
    - Where:
      - X/Twitter: https://twitter.com/hwchase17
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 10: Brian Roemmele
    - Who:
      - Independent AI researcher
      - Coined the term "AI whisperer"
    - Focus:
      - Creative persona prompting techniques
      - Conversational agent development
      - Novel prompting methodologies
    - Why Follow:
      - Explores novel, sometimes contrarian prompting styles useful for inspiration
      - Creative approach to human-AI interaction
    - Where:
      - X/Twitter: https://twitter.com/BrianRoemmele
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 11: Simon Willison
    - Who:
      - Open-source developer
      - Creator of Datasette
    - Focus:
      - Prompt injection security
      - LLM integration patterns
      - Security considerations in AI applications
    - Why Follow:
      - Writes deeply researched blogs on securing prompts and preventing jailbreaks
      - Critical focus on AI security and safety
    - Where:
      - Blog: https://simonwillison.net/
      - X/Twitter: https://twitter.com/simonw
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 12: Shunyu Yao
    - Who:
      - AI researcher at Anthropic (ex-Princeton)
      - Lead author of ReAct and Tree-of-Thoughts papers
    - Focus:
      - Tool-augmented reasoning
      - Search-based prompting
      - Agentic LLM development
    - Why Follow:
      - Publishes state-of-the-art research and open-source demos for advanced reasoning paradigms
      - Pioneer in combining reasoning with action in LLMs
    - Where:
      - Blog: https://ysymyth.github.io
      - GitHub: https://github.com/ShunyuYao
      - Papers: https://arxiv.org/pdf/2210.03629.pdf (ReAct), https://arxiv.org/pdf/2305.10601.pdf (Tree-of-Thoughts)
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 13: Jason Wei
    - Who:
      - Research Scientist at OpenAI (ex-Google Brain)
      - Co-author of Chain-of-Thought, Self-Consistency, and many prompting studies
    - Focus:
      - Emergent abilities in LLMs
      - Chain-of-thought prompting
      - LLM evaluation methodologies
    - Why Follow:
      - Shares insightful analysis on prompting discoveries and LLM scaling laws
      - Pioneer in chain-of-thought reasoning techniques
    - Where:
      - X/Twitter: https://twitter.com/jasonwei20
      - Personal site: https://www.jasonwei.net
      - Google Scholar: https://scholar.google.com/citations?user=MNB3H7AAAAAJ
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 14: Xuezhi Wang
    - Who:
      - Senior Staff Research Scientist at Google DeepMind
      - Lead author on "Self-Consistency Improves Chain-of-Thought Reasoning" and "LLMs as Optimizers"
    - Focus:
      - Robust reasoning techniques
      - Self-consistency decoding
      - Meta-optimization via prompting
    - Why Follow:
      - Invented decoding tricks that materially boost reasoning accuracy across tasks
      - Deep expertise in improving LLM reasoning reliability
    - Where:
      - Google Scholar: https://scholar.google.com/citations?user=ScLUQ-YAAAAJ
      - Research profile: https://research.google/people/105995/
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 15: Andrew N. Gao
    - Who:
      - Stanford PhD student
      - Creator of PAL (Program-Aided Language models)
    - Focus:
      - Code-generation-aided reasoning
      - Mathematical problem solving with LLMs
      - Hybrid symbolic-neural approaches
    - Why Follow:
      - Demonstrates how writing and running code inside prompts boosts logical accuracy
      - Innovation in combining programming with natural language reasoning
    - Where:
      - GitHub: https://github.com/andrewglv
      - Google Scholar: https://scholar.google.com/citations?user=A3A1p4UAAAAJ
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 16: Marek Besta
    - Who:
      - Researcher at ETH Zurich
      - Co-author of Graph-of-Thoughts paper
    - Focus:
      - Graph reasoning methodologies
      - Scalable LLM search frameworks
      - Advanced reasoning architectures
    - Why Follow:
      - Explores flexible graph-structured thought processes that surpass tree-search methods
      - Innovation in non-linear reasoning approaches
    - Where:
      - Google Scholar: https://scholar.google.com/citations?user=NnPjmCYAAAAJ
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 17: Lee Boonstra
    - Who:
      - Staff Developer Relations Engineer and AI Advocate at Google Cloud
      - Author of Google's "Prompt Engineering" white-paper (September 2024)
    - Focus:
      - Practical prompt-engineering techniques for Gemini/Vertex AI
      - LLM configuration (temperature, top-K/P)
      - Production best-practices for enterprise deployment
    - Why Follow:
      - Combines hands-on demos with enterprise guidance, offering checklists and battle-tested tips for shipping LLM apps faster and more safely
      - Bridge between Google's research and practical implementation
    - Where:
      - X/Twitter: https://twitter.com/LeeBoonstra
      - Blog: https://cloud.google.com/blog/authors/lee-boonstra
      - White-paper PDF: https://services.google.com/fh/files/misc/prompt_engineering_whitepaper.pdf
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 18: Chengrun Zhou
    - Who:
      - PhD researcher at Carnegie Mellon University
      - First author of "Automatic Prompt Engineer (APE)" paper
    - Focus:
      - Automated prompt search and evaluation
      - Black-box LLM optimization
      - Meta-learning for prompt engineering
    - Why Follow:
      - Shows how LLMs can write and score their own prompts, hinting at fully self-improving systems
      - Pioneer in automated prompt optimization
    - Where:
      - Paper: https://arxiv.org/pdf/2211.01910.pdf
      - GitHub (APE reference implementation): https://github.com/keirp/automatic_prompt_engineer
    - Key Insights:
      - Insight 1: {{To be filled based on additional research}}
      - Insight 2: {{To be filled based on additional research}}
    - Sources:
      - Source 1: {{To be filled based on research}}
  - Expert 19: God of Prompt
    - Who:
      - European-based AI educator & entrepreneur behind GodofPrompt.ai.
      - Curator of a 30 000-prompt library covering ChatGPT, Claude, Midjourney & Gemini.
      - Social reach: >50 K followers on X (Feedspot, Jun 2025) and 5.6 K on Facebook.
    - Focus:
      - Prompt-engineering best practices, meta-prompting, GPT-app productization, AI-driven content workflows.
    - Why Follow:
      - Publishes “mega-prompts” and toolkits that consistently improve LLM output quality.
      - Demonstrates the business side of prompts (claims $450 K in sales) and early moves (GPT Builder Toolkit) that anticipate platform shifts like the GPT Store.
    - Where:
      - X/Twitter – [https://x.com/godofprompt](https://x.com/godofprompt)
      - Blog & Prompt Library – [https://godofprompt.ai](https://godofprompt.ai/)
      - Newsletter – [https://godofprompt.beehiiv.com](https://godofprompt.beehiiv.com/)
      - LinkedIn – [https://www.linkedin.com/company/godofprompt](https://www.linkedin.com/company/godofprompt)
      - YouTube – [https://www.youtube.com/@GodofPrompt](https://www.youtube.com/@GodofPrompt)
    - Key Insights:
      - Insight 1: Structured, multi-phase prompts (role → context → constraints → iterate) outperform single-line requests.
      - Insight 2: Meta-prompting—asking AI to design or improve prompts—speeds iteration and boosts output fidelity.
      - Insight 3: Prompt assets are monetisable; packaged bundles and custom GPT templates can generate six-figure revenue streams.
      - Insight 4: Prompt engineering will evolve—not disappear—as custom GPT marketplaces expand and multimodal models demand richer instructions.
    - Sources
      - Source 1: “12 Best Practices for Prompt Engineering” – GodofPrompt.ai blog (Jan 2025)
        - Summary:
          - Enumerates core rules (be specific, add context, iterate, stay updated).
          - Positions prompt writing as “the new programming interface.”
        - Link:
          - [https://www.godofprompt.ai/blog/12-best-practices-for-prompt-engineering-must-know-tips](https://www.godofprompt.ai/blog/12-best-practices-for-prompt-engineering-must-know-tips)
      - Source 2: X thread – “Everyone says prompt engineering is dead…” (18 Apr 2024)
        - Summary:
          - Claims journey from $0 to $450 K selling prompts.
          - Shares a free “research-agent” mega-prompt positioned as a Perplexity replacement.
        - Link:
          - [https://twitter.com/godofprompt/status/1856704767273164928](https://twitter.com/godofprompt/status/1856704767273164928)
      - Source 3: Feedspot — “10 Best Prompt Engineering Blogs 2025” (11 Jun 2025)
        - Summary:
          - Ranks God of Prompt Blog #3 worldwide; cites 51.1 K X followers & 5.6 K Facebook followers.
        - Link:
          - [https://bloggers.feedspot.com/prompt_engineering_blogs/](https://bloggers.feedspot.com/prompt_engineering_blogs/)
      - Source 4: LinkedIn post – “Master Meta Prompting with Ease!” (Nov 2024)
        - Summary:
          - Explains using ChatGPT to draft, critique & refine prompts; 200+ reactions show professional traction.
        - Link:
          - [https://www.linkedin.com/posts/god-of-prompt_master-meta-prompting-with-ease-discover-activity-7264047014227632131-UwoK](https://www.linkedin.com/posts/god-of-prompt_master-meta-prompting-with-ease-discover-activity-7264047014227632131-UwoK)
      - Source 5: X post – GPT Builder Toolkit launch (4 Jan 2024)
        - Summary:
          - Markets “100+ mega-instructions for GPT building” ahead of OpenAI’s GPT Store; illustrates rapid trend-spotting.
        - Link:
          - [https://twitter.com/godofprompt/status/1745396684408959304](https://twitter.com/godofprompt/status/1745396684408959304)
- DOK 4: SpikyPOVs
- DOK 3: Insights
  - Insight 1: Production Prompts Aren't Conversations; They're Machine-Readable APIs.
    - Synthesis: Across the most effective techniques from both OpenAI (Category 1) and Anthropic (Category 8, 11), a clear pattern emerges: the most reliable prompts abandon natural language ambiguity in favor of rigid, computer-parsable structures. Techniques like using XML tags (<document>, <instructions>), explicit delimiters (###), JSON schemas for output, and giving a role via system prompts are not just helpful tips—they are a paradigm shift. They transform the prompt from a loose request into a well-defined API contract. Frameworks like LangChain (Category 3) and Guidance (Category 3) institutionalize this by treating prompts as version-controlled code templates with typed inputs and constrained outputs.
    - Strategic Implication: Stop treating prompting as an exercise in creative writing and start treating it as software engineering. Prompts must be versioned in Git, unit-tested with evaluation frameworks (Category 12), and modularized into reusable templates. The goal is to minimize the model's "interpretive dance" by defining a strict data-in, data-out contract. This dramatically reduces hallucinations, ensures consistent formatting for downstream processing, and makes the system resilient to model updates.
  - Insight 2: Reasoning Isn't a Single Prompt Trick; It's an Evolving Computational Architecture.
    - Synthesis: The initial discovery of "Chain-of-Thought" (CoT) (Category 4, 14) by simply adding "Let's think step-by-step" was just the beginning. The research clearly shows that for complex problems, a single line of reasoning is brittle. Advanced techniques build on this by creating a "portfolio of thought." Self-Consistency (Category 4) generates multiple reasoning paths and takes a majority vote. Tree-of-Thoughts and Graph-of-Thoughts (Category 4) upgrade this from a simple vote to a deliberate search process, allowing the model to explore, evaluate, and backtrack from different reasoning branches. This is further reinforced by self-correction loops (Category 7, 10), where a model critiques its own reasoning and tries again.
    - Strategic Implication: For any high-stakes or complex task, relying on a basic zero-shot CoT prompt is a liability. Production systems must implement more robust reasoning architectures. Instead of asking the model to simply "think," the prompt should orchestrate a search process. This means prompting the model to generate multiple possible plans, evaluate them against success criteria, and then execute the best one. The focus shifts from getting one perfect answer to designing a system that can explore a solution space and converge on the most robust conclusion.
  - Insight 3: The Most Effective Prompts Outsource Hard Work, Turning the LLM into an Orchestrator, Not a Doer.
    - Synthesis: A dominant theme across advanced applications is mitigating LLM weaknesses by offloading tasks to more reliable tools. Retrieval-Augmented Generation (RAG) (Category 6, 16) outsources knowledge recall to a database, eliminating the need for the model to memorize facts. Program-Aided/Program-of-Thoughts (PAL/PoT) (Category 5) outsources calculation and logical execution to a code interpreter, eliminating math errors. The ReAct framework (Category 5, 15) provides the "master pattern" for this, creating a loop where the model reasons about which action to take (e.g., search, calculate, lookup), executes it via a tool, and then incorporates the result back into its reasoning.
    - Strategic Implication: The primary goal of a production prompt should be to delegate. Your engineering effort should be focused less on finding the perfect adjective and more on building a robust, well-documented set of tools (APIs, databases, code interpreters) the LLM can use. The prompt then becomes the "operating manual" that teaches the LLM how to be a smart orchestrator—when to use the calculator, when to search the knowledge base, and how to synthesize the results. This approach contains hallucinations and produces more accurate, auditable, and factually grounded outputs.
  - Insight 4: The Ultimate Prompt Engineering Lever is Using LLMs to Optimize and Police Themselves.
    - Synthesis: The most advanced research moves beyond humans manually crafting prompts. Techniques like Automatic Prompt Engineer (APE) (Category 7) use a powerful LLM to discover prompts that work best for a specific task, often outperforming human-written ones. Similarly, self-correction patterns like Self-Refine (Category 7) and chained reviews (Category 10) use the LLM to act as its own critic, iteratively improving its output. This "meta-prompting" extends to evaluation, where models like GPT-4 are used as reliable, scalable judges of other models' outputs (G-Eval, Category 12).
    - Strategic Implication: Implement a two-tiered development workflow. Use a powerful, expensive "architect" model (e.g., Claude 3 Opus, GPT-4o) for the meta-task of generating, critiquing, and refining prompts. These optimized prompts can then be deployed with cheaper, faster "worker" models in production. This creates a flywheel: the architect model builds a test suite (using tools like OpenAI Evals, Category 12) and then optimizes prompts to pass it, reducing manual labor, cutting inference costs, and systematically improving quality. The human's role evolves from writer to the designer of this automated optimization system.
  - Insight 5: Context Engineering represents a fundamental paradigm shift from prompt engineering to comprehensive system design, where the focus moves from crafting perfect instructions to building dynamic information ecosystems that provide AI models with everything they need to accomplish tasks. This shift recognizes that the "magic" in AI agents comes not from clever prompts or advanced models, but from the quality and comprehensiveness of context provided before the LLM generates its response.
  - Insight 6: The context window's comprehensive information architecture can enhance LLM performance more significantly than optimized prompts or sophisticated tools operating in isolation. While prompt engineering focuses on perfecting instruction phrasing and tool selection emphasizes capability expansion, Context Engineering demonstrates that providing rich, relevant context—including user history, retrieved information, available tools, and structured output definitions—creates a multiplicative effect that transforms basic demos into "magical" AI products, as illustrated by the meeting scheduling example where contextual awareness of calendars, past communications, and relationships enabled natural, helpful responses.
  - Insight 7: Most AI agent failures are context failures rather than model failures, fundamentally changing how we should approach AI system reliability and debugging. This insight challenges the common assumption that improving AI systems requires better models or more sophisticated algorithms, instead positioning context quality as the primary determinant of agent success or failure, which shifts engineering focus toward information architecture and context management systems.
  - Insight 8: Context Engineering operates as a cross-functional discipline that requires understanding business use cases, defining structured outputs, and architecting information flow, making it fundamentally different from traditional prompt engineering's linguistic focus. The seven components of context—instructions/system prompt, user prompt, state/history, long-term memory, retrieved information (RAG), available tools, and structured output definitions—create a comprehensive framework that demands expertise across multiple domains rather than just prompt crafting skills.
  - Insight 9: The concept of "limited working memory" in AI agents makes context selection and prioritization a critical engineering challenge analogous to memory management in traditional computing systems. With the rise of AI agents, the strategic curation of what information gets loaded into this limited working memory becomes paramount, requiring sophisticated systems that can dynamically determine the right information, tools, and format at the right time for each specific task.
  - Insight 10: Context Engineering transforms AI development from an art of prompt crafting into a measurable engineering discipline with systematic approaches to information architecture. Rather than relying on intuitive prompt writing, Context Engineering provides a structured methodology for building systems that gather, format, and present information to LLMs, enabling more predictable and scalable AI agent development with clear inputs, processes, and measurable outputs.
  - Insight 11: Large Language Models fundamentally violate the uniform token processing assumption, with performance varying significantly as input length changes even on simple tasks. This challenges the core premise that models handle the 10,000th token as reliably as the 100th token, revealing systematic degradation in processing capability across extended contexts.
  - Insight 12: Traditional Needle in a Haystack (NIAH) assessments severely underestimate real-world context processing complexity by focusing only on direct lexical matching rather than semantic reasoning. The NoLiMa benchmark extension reveals that 72.4% of needle-question pairs require external world knowledge, making context processing a dual task of retrieval and reasoning rather than simple pattern matching.
  - Insight 13: Semantic interference creates a counterintuitive processing dynamic where models perform better when needles do not semantically blend with their haystack context. This means that stuffing entire codebases into large context windows, despite technical capability, represents a massive antipattern that degrades rather than enhances model performance.
  - Insight 14: Structural coherence in context creates a double-edged processing effect where logically structured haystacks make needles harder to find compared to shuffled, random content. Across all 18 models tested, consistent patterns show better performance on shuffled haystacks than coherent essays, suggesting that logical flow actually interferes with attention mechanisms.
  - Insight 15: Distractor elements compound performance degradation non-uniformly, with even a single distractor reducing performance relative to baseline and four distractors causing compounded decline. Critically, different distractors have varying impact levels, indicating that attention mechanisms are sensitive to specific content characteristics rather than processing all interference equally.
  - Insight 16: Long context utilization for chat assistants requires models to perform dual cognitive tasks simultaneously - finding relevant parts of conversation history (retrieval) and synthesizing them meaningfully for incoming queries (reasoning). This dual-task burden explains why focused contexts often outperform maximum context utilization, even in high-powered thinking models like Claude Opus 4.
  - Insight 17: Models systematically fail at positional indexing tasks with repeated elements, unable to accurately identify specific instances of words within sequences of identical terms. This fundamental limitation highlights the critical importance of using deterministic tooling for seemingly trivial tasks like index identification, as LLMs cannot reliably track positional relationships in repetitive contexts.
  - Insight 18: The "more context is better" paradigm represents a fundamental misunderstanding of attention mechanism limitations, where strategic context curation consistently outperforms context maximization. Input length directly correlates with increasing non-uniformity in performance, suggesting that context windows should be treated as scarce resources requiring careful optimization rather than repositories for maximum information dumping.
- DOK 1 & 2: Knowledge Tree/Categories
  - Category 1: Prompt Engineering Best Practices & Techniques
    - Summary:
      - Foundational guidelines and concrete tactics from LLM providers and industry blogs for crafting clear, reliable, and safe prompts.
    - Sources
      - OpenAI — “Best practices for prompt engineering with the OpenAI API”
        - Summary:
          - Official rules‑of‑thumb (use latest model, put instructions first, be explicit on format).
          - Includes examples with delimiters, step‑by‑step reasoning, and JSON output patterns.
        - Link:
          - [https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api)
        - Insights:
          - Use the latest model.
            - For best results, we generally recommend using the latest, most capable models. Newer models tend to be easier to prompt engineer.
          - Put instructions at the beginning of the prompt and use `###`  or `"""`  to separate the instruction and context.
            - Less effective:
              - ```
Summarize the text below as a bullet point list of the most important points.

{text input here}
```
            - More effective:
              - ```
Summarize the text below as a bullet point list of the most important points.

Text: """
{text input here}
"""
```
          - Be specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc.
            - Less effective: `Write a poem about OpenAI.` 
            - More effective: `Write a short inspiring poem about OpenAI, focusing on the recent DALL-E product launch (DALL-E is a text to image ML model) in the style of a {famous poet}.`
          - Articulate the desired output format through examples.
            - Less effective:
              - ```
Extract the entities mentioned in the text below. Extract the following 4 entity types: company names, people names, specific topics and themes.

Text: {text}
```
            - More effective:
              - ```
Extract the important entities mentioned in the text below. First extract all company names, then extract all people names, then extract specific topics which fit the content and finally extract general overarching themes

Desired format:
Company names: <comma_separated_list_of_company_names>
People names: -||-
Specific topics: -||-
General themes: -||-

Text: {text}
```
              - Show, and tell - the models respond better when shown specific format requirements. This also makes it easier to programmatically parse out multiple outputs reliably.
          - Reduce “fluffy” and imprecise descriptions.
            - Less effective: `The description for this product should be fairly short, a few sentences only, and not too much more.` 
            - More effective: `Use a 3 to 5 sentence paragraph to describe this product.` 
          - Instead of just saying what not to do, say what to do instead.
            - Less effective:
              - ```
The following is a conversation between an Agent and a Customer. DO NOT ASK USERNAME OR PASSWORD. DO NOT REPEAT.

Customer: I can’t log in to my account.
Agent:
```
            - More effective:
              - ```
The following is a conversation between an Agent and a Customer. The agent will attempt to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq

Customer: I can’t log in to my account.
Agent:
```
          - Code Generation Specific - Use “leading words” to nudge the model toward a particular pattern.
            - Less effective:
              - ```
# Write a simple python function that
# 1. Ask me for a number in mile
# 2. It converts miles to kilometers
```
            - More effective:
              - ```
# Write a simple python function that
# 1. Ask me for a number in mile
# 2. It converts miles to kilometers

import
```
          - Parameters:
            - `model` - Higher performance [models](https://platform.openai.com/docs/models) are generally more expensive and may have higher latency.
            - `temperature` - A measure of how often the model outputs a less likely token. The higher the `temperature`, the more random (and usually creative) the output. This, however, is not the same as “truthfulness”. For most factual use cases such as data extraction, and truthful Q&A, the `temperature` of 0 is best.
            - `max_completion_tokens` (maximum length) - Does not control the length of the output, but a hard cutoff limit for token generation. Ideally you won’t hit this limit often, as your model will stop either when it thinks it’s finished, or when it hits a stop sequence you defined.
            - `stop` (stop sequences) - A set of characters (tokens) that, when generated, will cause the text generation to stop.
      - Anthropic — “Prompt Engineering Overview”
        - Summary:
          - Be explicit and detailed: Provide clear instructions, context (purpose, audience, desired outcome), and specific examples to guide Claude, as seen in creating effective marketing emails or incident reports.
          - Use XML-style tags: Structure prompts with tags like <instructions>, <example>, <document>, and <thinking> to separate components and control output format.
          - Employ few-shot prompting: Provide multiple relevant and diverse examples, wrapped in tags, to guide Claude's behavior and achieve desired structured outputs, such as for data anonymization.
          - Encourage step-by-step reasoning: Use Chain of Thought (CoT) prompting (e.g., "think step-by-step," guided steps, or <thinking> tags) for complex tasks like financial analysis, and leverage "extended thinking" for deeper problem-solving.
          - Assign roles via system prompts: Define a specific persona for Claude (e.g., "You are a CFO") to tailor its tone, focus, and expertise for the task.
          - Chain prompts for complex workflows: Break down intricate, multi-step tasks (like legal review and email drafting, or self-correction cycles) into sequential sub-prompts, passing outputs between them.
          - Optimize for long context: When using large documents, place them at the beginning of the prompt, use XML tags to structure multi-document inputs, and ask Claude to cite sources with quotes.
          - Iterate and refine: Continuously test and improve prompts, potentially using tools like a "prompt improver," to achieve optimal results.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
        - Insights:
          - Be explicit with your instructions. Models respond well to clear, explicit instructions. Being specific about your desired output can help enhance results.
            - Example: Creating an analytics dashboard
              - Less effective: `Create an analytics dashboard` 
              - More effective: `Create an analytics dashboard. Include as many relevant features and interactions as possible. Go beyond the basics to create a fully-featured implementation.` 
          - Add context to improve performance. Providing context or motivation behind your instructions, such as explaining to Claude why such behavior is important, can help Claude 4 better understand your goals and deliver more targeted responses.
            - Example: Formatting preferences
              - Less effective: `NEVER use ellipses` 
              - More effective: `Your response will be read aloud by a text-to-speech engine, so never use ellipses since the text-to-speech engine will not know how to pronounce them.` 
            - Be vigilant with examples and details. Claude 4 models pay attention to details and examples as part of instruction following. Ensure that your examples align with the behaviors you want to encourage and minimize behaviors you want to avoid.
          - Control the format of responses.
            - Example: Tell Claude what to do instead of what not to do
              - Instead of: `Do not use markdown in your response` 
              - Try: `Your response should be composed of smoothly flowing prose paragraphs.` 
            - Example: Use XML format indicators
              - Try: `Write the prose sections of your response in <smoothly_flowing_prose_paragraphs> tags.` 
            - Match your prompt style to the desired output. The formatting style used in your prompt may influence Claude’s response style. If you are still experiencing steerability issues with output formatting, we recommend as best as you can matching your prompt style to your desired output style. For exmaple, removing markdown from your prompt can reduce the volume of markdown in the output.
          - Utilize the prompt improver to optimize your prompts.
            - The prompt improver helps you quickly iterate and improve your prompts through automated analysis and enhancement. It excels at making prompts more robust for complex tasks that require high accuracy.
            - Example:
              - Note that this example applies to **all** prompts, not just prompts used with Claude.
              - Original prompt:
                - ```
From the following list of Wikipedia article titles, identify which article this sentence came from.
Respond with just the article title and nothing else.

Article titles:
{{titles}}

Sentence to classify:
{{sentence}}

```
              - Improved prompt:
                - ```
You are an intelligent text classification system specialized in matching sentences to Wikipedia article titles. Your task is to identify which Wikipedia article a given sentence most likely belongs to, based on a provided list of article titles.

First, review the following list of Wikipedia article titles:
<article_titles>
{{titles}}
</article_titles>

Now, consider this sentence that needs to be classified:
<sentence_to_classify>
{{sentence}}
</sentence_to_classify>

Your goal is to determine which article title from the provided list best matches the given sentence. Follow these steps:

1. List the key concepts from the sentence
2. Compare each key concept with the article titles
3. Rank the top 3 most relevant titles and explain why they are relevant
4. Select the most appropriate article title that best encompasses or relates to the sentence's content

Wrap your analysis in <analysis> tags. Include the following:
- List of key concepts from the sentence
- Comparison of each key concept with the article titles
- Ranking of top 3 most relevant titles with explanations
- Your final choice and reasoning

After your analysis, provide your final answer: the single most appropriate Wikipedia article title from the list.

Output only the chosen article title, without any additional text or explanation.
```
          - Be clear, direct, and detailed.
            - Give Claude contextual information: Just like you might be able to better perform on a task if you knew more context, Claude will perform better if it has more contextual information. Some examples of contextual information. 
              - What the task results will be used for
              - What audience the output is meant for
              - What workflow the task is a part of, and where this task belongs in that workflow
              - The end goal of the task, or what a successful task completion looks like
            - Be specific about what you want Claude to do: For example, if you want Claude to output only code and nothing else, say so.
            - Provide instructions as sequential steps: Use numbered lists or bullet points to better ensure that Claude carries out the task the exact way you want it to.
            - Example: Anonymizing customer feedback
              - Notice that Claude still makes mistakes in the unclear prompting example, such as leaving in a customer’s name.
              - Role: User
                - Unclear Prompt: 
                  - ```
`Please remove all personally identifiable information from these customer feedback messages: {{FEEDBACK_DATA}}` 
```
                - Clear Prompt:
                  - ```
Your task is to anonymize customer feedback for our quarterly review.

Instructions:
1. Replace all customer names with “CUSTOMER_[ID]” (e.g., “Jane Doe” → “CUSTOMER_001”).
2. Replace email addresses with “EMAIL_[ID]@example.com”.
3. Redact phone numbers as “PHONE_[ID]“.
4. If a message mentions a specific product (e.g., “AcmeCloud”), leave it intact.
5. If no PII is found, copy the message verbatim.
6. Output only the processed messages, separated by ”---”.

Data to process: {{FEEDBACK_DATA}}
```
              - Role: Assistant (Claude's response)
                - Unclear Prompt:
                  - ```
Here are the anonymized customer feedback messages:
1. I’ve been using your software for years, and it’s been great. But lately, AcmeCloud has been crashing a lot. Fix it!
2. XXX, your customer support rep, was incredibly helpful. He guided me through the whole process. My email is XXX if you want to send a survey.
3. I’m John Smith, and I’m not happy. I’ve been on hold for 2 hours trying to reach someone at XXX. This is unacceptable!
```
                - Clear Prompt:
                  - ```
CUSTOMER_001: I’ve been using your software for years, and it’s been great. But lately, AcmeCloud has been crashing a lot. Fix it!
---
CUSTOMER_002: CUSTOMER_SERVICE_001, your customer support rep, was incredibly helpful. He guided me through the whole process. My email is [EMAIL_002@example.com](mailto:EMAIL_002@example.com) if you want to send a survey.
---
CUSTOMER_003: I’m CUSTOMER_003, and I’m not happy. I’ve been on hold for 2 hours trying to reach someone at PHONE_001. This is unacceptable!
```
            - Example: Crafting a marketing email campaign
              - Notice that Claude makes up details to fill in the gaps where it lacks context with the vague prompt.
              - Role: User
                - Vague Prompt:
                  - ```
Write a marketing email for our new AcmeCloud features.
```
                - Specific Prompt:
                  - ```
Your task is to craft a targeted marketing email for our Q3 AcmeCloud feature release.

Instructions:
1. Write for this target audience: Mid-size tech companies (100-500 employees) upgrading from on-prem to cloud.
2. Highlight 3 key new features: advanced data encryption, cross-platform sync, and real-time collaboration.
3. Tone: Professional yet approachable. Emphasize security, efficiency, and teamwork.
4. Include a clear CTA: Free 30-day trial with priority onboarding.
5. Subject line: Under 50 chars, mention “security” and “collaboration”.
6. Personalization: Use {{COMPANY_NAME}} and {{CONTACT_NAME}} variables.

Structure:
1. Subject line
2. Email body (150-200 words)
3. CTA button text
```
              - Role: Assistant (Claude's response)
                - Vague Prompt:
                  - ```
Here’s a draft marketing email for our new features:

Subject: Boost Your Business with AcmeCloud’s Latest Features

Dear Valued Customer,

We’re excited to introduce three powerful features to AcmeCloud that will revolutionize your workflow:

1. Real-Time Collaboration: Work on documents simultaneously with your team. No more version control issues!
2. AI-Powered Insights: Make data-driven decisions with AI that analyzes trends and automates tasks.
3. Advanced Security Suite: End-to-end encryption, multi-factor auth, and threat detection keep your data safe.

Limited Offer: Upgrade now for a 20% first-year discount. Plus, refer a friend for an extra 10% off!

Best,
The AcmeCloud Team
```
                - Specific Prompt:
                  - ```
Subject line: AcmeCloud: Enterprise security meets real-time teamwork

Email body:
Dear {{CONTACT_NAME}},

At {{COMPANY_NAME}}, you’re scaling fast and eyeing the cloud. But with growth comes the challenge of keeping data secure while fostering collaboration. That’s where AcmeCloud’s Q3 release comes in, tailor-made for tech innovators like you.

1. Fort Knox for Your Files: Military-grade encryption now safeguards your data, both in motion and at rest. Your intellectual property stays yours, period.
2. Sync Without Borders: Windows, Mac, or Linux—AcmeCloud speaks all languages. Unified version control means no more “final_v2_REALLY_FINAL.doc”.
3. Collaborate at Light Speed: Ditch the attachment ping-pong. Now your devs, designers, and data scientists co-create in real-time, slashing time-to-market.

Don’t just take our word for it. Experience the AcmeCloud difference with our 30-day trial. And because we know time is your most precious asset, we’re throwing in priority onboarding.

Elevate Your Team Now - Free 30-Day Trial
```
            - Example: Incident response
              - Notice that Claude outputs superfluous text and different formating with the vague prompt.
              - Role: User
                - Vague Prompt:
                  - ```
Analyze this AcmeCloud outage report and summarize the key points.

{{REPORT}}

```
                - Detailed Prompt:
                  - ```
Analyze this AcmeCloud outage report. Skip the preamble. Keep your response terse and write only the bare bones necessary information. List only:
1) Cause
2) Duration
3) Impacted services
4) Number of affected users
5) Estimated revenue loss.

Here’s the report: {{REPORT}}
```
              - Role: Assistant (Claude's response)
                - Vague Prompt:
                  - ```
Here’s a summary of the key points from the AcmeCloud outage report:

On May 15th, a misconfigured load balancer update caused significant traffic routing issues, resulting in a service outage from 09:15 to 13:52 UTC. The 4-hour 37-minute disruption impacted our core file syncing service and admin dashboard, rendering them inaccessible. Additionally, the mobile app experienced partial functionality loss. Approximately 68% of our user base, totaling 1.3 million users, were affected. Based on our Service Level Agreements (SLAs) and average revenue per user, we estimate a financial impact of $420,000 in service credits and potential customer churn.
```
                - Detailed Prompt:
                  - ```
1) Cause: Misconfigured load balancer update
2) Duration: 4h 37m (09:15-13:52 UTC, May 15)
3) Impacted: Core sync, admin dashboard (down); mobile app (partial)
4) Affected users: 1.3M (68% of base)
5) Est. revenue loss: $420,000
```
          - Use examples (multi-shot prompting) to guide Claude's behavior.
            - Examples are your secret weapon shortcut for getting Claude to generate exactly what you need. By providing a few well-crafted examples in your prompt, you can dramatically improve the accuracy, consistency, and quality of Claude’s outputs. This technique, known as few-shot or multishot prompting, is particularly effective for tasks that require structured outputs or adherence to specific formats.
            - Why use examples?
              - Accuracy: Examples reduce misinterpretation of instructions.
              - Consistency: Examples enforce uniform structure and style.
              - Performance: Well-chosen examples boost Claude's ability to handle complex tasks.
            - Crafting effective examples:
              - Relevant: Your examples mirror your actual use case.
              - Diverse: Your examples cover edge cases and potential challenges, and vary enough that Claude doesn't inadvertently pick up on unintended patterns.
              - Clear: Your examples are wrapped in `<example>`  tags (if multiple, nested within `<examples>`  tags) for structure.
          - Let Claude think (chain of thought prompting to increase performance.
            - When faced with complex tasks like research, analysis, or problem-solving, giving Claude space to think can dramatically improve its performance. This technique, known as chain of thought (CoT) prompting, encourages Claude to break down problems step-by-step, leading to more accurate and nuanced outputs.
            - Use CoT for tasks that a human would need to think through, like complex math, multi-step analysis, writing complex documents, or decisions with many factors.
            - Why let Claude think?
              - Accuracy: Stepping through problems reduces errors, especially in math, logic, analysis, or generally complex tasks.
              - Coherence: Structured thinking leads to more cohesive, well-organized responses.
              - Debugging: Seeing Claude’s thought process helps you pinpoint where prompts may be unclear.
            - Why not let Claude think?
              - Increased output length may impact latency.
              - Not all tasks require in-depth thinking. Use CoT judiciously to ensure the right balance of performance and latency.
            - How to prompt for thinking.
              - The chain of thought techniques below are ordered from least to most complex. Less complex methods take up less space in the context window, but are also generally less powerful.
              - Basic prompt: Include “Think step-by-step” in your prompt. 
                - Lacks guidance on how to think (which is especially not ideal if a task is very specific to your app, use case, or organization)
                - Example: Writing donor emails (basic CoT)
                  - ```
Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think step-by-step before you write the email.
```
              - Guided prompt: Outline specific steps for Claude to follow in its thinking process.
                - Lacks structuring to make it easy to strip out and separate the answer from the thinking.
                - Example: Writing donor emails (guided CoT)
                  - ```
Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think before you write the email. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email using your analysis.
```
              - Structured prompt:  Use XML tags like `<thinking>` and `<answer>` to separate reasoning from the final answer.
                - Example: Writing donor emails (structured guided CoT)
                  - ```
Draft personalized emails to donors asking for contributions to this year’s Care for Kids program.

Program information:
<program>{{PROGRAM_DETAILS}}
</program>

Donor information:
<donor>{{DONOR_DETAILS}}
</donor>

Think before you write the email in <thinking> tags. First, think through what messaging might appeal to this donor given their donation history and which campaigns they’ve supported in the past. Then, think through what aspects of the Care for Kids program would appeal to them, given their history. Finally, write the personalized donor email in <email> tags, using your analysis.

```
            - Example: Financial analysis without thinking
              - Without step-by-step thinking, Claude gives a recommendation that seems reasonable but lacks depth. It doesn’t quantify the potential outcomes or consider historical market performance, which are crucial for making an informed decision.
              - Role: User
                - Content:
                  - ```
You’re a financial advisor. A client wants to invest $10,000. They can choose between two options: A) A stock that historically returns 12% annually but is volatile, or B) A bond that guarantees 6% annually. The client needs the money in 5 years for a down payment on a house. Which option do you recommend?

```
              - Role: Assistant
                - Content:
                  - ```
Based on the information provided, I would recommend option B, the bond that guarantees a 6% annual return. Here’s why:

1. Certainty: The bond offers a guaranteed 6% annual return, which provides certainty in financial planning. This is crucial when the client has a specific financial goal (down payment) with a fixed timeline (5 years).
2. Risk aversion: Since the client needs this money for an important life event (buying a house), they likely have a low risk tolerance. The volatility of the stock market could jeopardize their plans if there’s a downturn close to when they need the funds.
3. Time horizon: With only 5 years, there’s less time to recover from potential stock market downturns. Longer investment horizons (10+ years) are generally better suited for weathering stock market volatility.

While the stock’s historical 12% return is attractive, past performance doesn’t guarantee future results. The guaranteed 6% from the bond ensures the client will have the funds they need when it’s time to make the down payment.
```
            - Example: Financial analysis with thinking
              - With step-by-step thinking, the advisor provides a much richer analysis. It calculates exact figures for both scenarios, considers historical market volatility, and deeply analyzes the client’s risk tolerance. This thorough reasoning leads to a more confident and justifiable recommendation.
              - Role: User
                - Content:
                  - ```
You’re a financial advisor. A client wants to invest $10,000. They can choose between two options: A) A stock that historically returns 12% annually but is volatile, or B) A bond that guarantees 6% annually. The client needs the money in 5 years for a down payment on a house. Which option do you recommend? Think step-by-step.
```
              - Role: Assistant
                - Content:
                  - ```
To determine the best investment option for our client, I’ll consider several factors step-by-step:

<thinking>
1. Understand the client’s goal and timeline:
- Goal: Down payment for a house
- Timeline: 5 years
- Implication: This is a short-term, high-priority financial goal. The client likely cannot afford significant risk or volatility.

2. Analyze Option A (Stock):
- Pro: Higher historical return (12% annually)
- Con: Volatile (implying potential for significant gains or losses)
- Calculate potential 5-year return:
- Best case (12% annually): $10,000 * (1.12)^5 ≈ $17,623.42
- Worst case (market crash): Could lose a substantial portion, e.g., 50% crash = $5,000

3. Analyze Option B (Bond):
- Pro: Guaranteed return (6% annually)
- Con: Lower return compared to historical stock performance
- Calculate 5-year return:
- Guaranteed: $10,000 * (1.06)^5 ≈ $13,382.25

4. Consider risk tolerance:
- Goal is a house down payment (essential, not discretionary)
- Short timeline (5 years) limits ability to recover from market downturns
- Conclusion: Client likely has low risk tolerance

5. Historical perspective:
- Stock’s 12% is historical, not guaranteed
- In 5-year periods, stock market can vary wildly:
- 2003-2007 (pre-financial crisis): S&P 500 up ~82%
- 2007-2011 (during crisis): S&P 500 down ~2%
- Conclusion: Past performance doesn’t guarantee future results, especially short-term

6. Final analysis:
- Option A (Stock): Higher potential return but unacceptable risk of not meeting the down payment goal
- Option B (Bond): Guaranteed to meet ~80% of best-case stock scenario, 100% certainty of having funds
</thinking>

<answer>
I recommend Option B, the bond with a guaranteed 6% annual return. While the stock’s historical 12% return is tempting, the bond ensures you’ll have $13,382.25 in 5 years for your house down payment. Given the importance and short timeline of your goal, the stock’s volatility poses an unacceptable risk. The bond provides certainty, which is invaluable for such a crucial financial milestone.
</answer>
```
          - Use XML tags to structure your prompts.
            - When your prompts involve multiple components like context, instructions, and examples, XML tags can be a game-changer. They help Claude parse your prompts more accurately, leading to higher-quality outputs.
            - Use tags like `<instructions>`, `<example>`, and `<formatting>` to clearly separate different parts of your prompt. This prevents Claude from mixing up instructions with examples or context.
            - Why use XML tags?
              - Clarity: Clearly separate different parts of your prompt and ensure your prompt is well structured.
              - Accuracy: Reduce errors caused by Claude misinterpreting parts of your prompt.
              - Flexibility: Easily find, add, remove, or modify parts of your prompt without rewriting everything.
              - Parseability: Having Claude use XML tags in its output makes it easier to extract specific parts of its response by post-processing.
            - Tagging best practices:
              - Be consistent: Use the same tag names throughout your prompts, and refer to those tag names when talking about the content (e.g, `Using the contract in <contract> tags...`).
              - Nest tags: You should nest tags `<outer><inner></inner></outer>` for hierarchical content.
            - Combine XML tags with other techniques like multishot prompting (`<examples>`) or chain of thought (`<thinking>`, `<answer>`). This creates super-structured, high-performance prompts.
            - Example: Generating financial reports
              - Without XML tags, Claude misunderstands the task and generates a report that doesn’t match the required structure or tone. After substitution, there is also a chance that Claude misunderstands where one section (like the the Q1 report example) stops and another begins.
              - Role: User
                - No XML Tags:
                  - ```
You’re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors. Include sections on Revenue Growth, Profit Margins, and Cash Flow, like with this example from last year: {{Q1_REPORT}}. Use data points from this spreadsheet: {{SPREADSHEET_DATA}}. The report should be extremely concise, to the point, professional, and in list format. It should and highlight both strengths and areas for improvement.
```
                - With XML Tags:
                  - ```
You’re a financial analyst at AcmeCorp. Generate a Q2 financial report for our investors.

AcmeCorp is a B2B SaaS company. Our investors value transparency and actionable insights.

Use this data for your report:<data>{{SPREADSHEET_DATA}}</data>

<instructions>
1. Include sections: Revenue Growth, Profit Margins, Cash Flow.
2. Highlight strengths and areas for improvement.
</instructions>

Make your tone concise and professional. Follow this structure:
<formatting_example>{{Q1_REPORT}}</formatting_example>
```
              - Role: Assistant
                - No XML Tags:
                  - ```
Q2 Financial Report for AcmeCorp Investors

Dear Valued Investors,

I am pleased to present our Q2 financial report, showcasing AcmeCorp’s continued growth and resilience in the B2B SaaS market.

1. Revenue Growth:
Our Q2 revenue reached $15.2M, a 22% YoY increase. This growth is primarily driven by a 30% expansion in our enterprise client base. However, our SMB segment saw a slight decline of 5%, which we aim to address through targeted marketing initiatives in Q3.

2. Profit Margins:
Gross margin improved to 72% (up 3% from Q1) due to infrastructure optimizations. EBITDA margin, however, dipped to 18% (down 2%) as we invested heavily in R&D for our upcoming AI-driven features.

3. Cash Flow:
Operating cash flow remains strong at $4.1M. Our cash reserves stand at $28M, providing a 15-month runway at current burn rate. We did see a 20% increase in customer acquisition costs, which we’re addressing through more efficient sales strategies.

In summary, AcmeCorp’s core financials remain robust. We’re balancing growth investments with prudent cash management. Our focus for Q3 will be revitalizing SMB growth and optimizing sales efficiency.

Thank you for your continued trust.

Sincerely,
CFO, AcmeCorp
```
                - With XML Tags:
                  - ```
Q2 Financial Report for AcmeCorp Investors</output>

Revenue Growth:
- Q2 revenue: $15.2M (22% YoY ↑)
- Enterprise clients: 30% ↑
- SMB segment: 5% ↓
Action: Targeted Q3 marketing

Profit Margins:
- Gross margin: 72% (3% ↑ from Q1)
Reason: Infrastructure optimizations
- EBITDA margin: 18% (2% ↓)
Reason: R&D investment in AI features

Cash Flow:
- Operating cash flow: $4.1M (strong)
- Cash reserves: $28M (15-month runway)
- Customer acquisition cost: 20% ↑
Action: Optimize sales strategies

Outlook:
- Core financials: Robust
- Q3 focus: Revitalize SMB, optimize sales

Management remains committed to balancing growth investments with financial discipline.
```
            - Example: Legal contract analysis
              - Without XML tags, Claude’s analysis is disorganized and misses key points. With tags, it provides a structured, thorough analysis that a legal team can act on.
              - Role: User
                - No XML Tags:
                  - ```
Analyze this software licensing agreement for potential risks and liabilities: {{CONTRACT}}. Focus on indemnification, limitation of liability, and IP ownership clauses. Also, note any unusual or concerning terms. Here’s our standard contract for reference: {{STANDARD_CONTRACT}}. Give a summary of findings and recommendations for our legal team.
```
                - With XML Tags:
                  - ```
Analyze this software licensing agreement for legal risks and liabilities.

We’re a multinational enterprise considering this agreement for our core data infrastructure.

<agreement>
{{CONTRACT}}</agreement>

This is our standard contract for reference:
<standard_contract>{{STANDARD_CONTRACT}}</standard_contract>

<instructions>
1. Analyze these clauses:
- Indemnification
- Limitation of liability
- IP ownership

2. Note unusual or concerning terms.

3. Compare to our standard contract.

4. Summarize findings in <findings> tags.

5. List actionable recommendations in <recommendations> tags.</instructions>
```
              - Role: Assistant
                - No XML Tags:
                  - ```
The provided software licensing agreement presents several concerning aspects. The indemnification clause is overly broad, potentially exposing us to significant liabilities. The limitation of liability section caps damages at a mere $500, which is inadequate for our enterprise-level usage. Regarding IP, the agreement grants the vendor joint ownership of any modifications or integrations we create.

In summary, this agreement is not in our best interest. It exposes us to undue risk, limits our recourse in case of failures, and compromises our intellectual property rights. I recommend renegotiating these terms or seeking alternative vendors.
```
                - With XML Tags:
                  - ```
<findings>
1. Indemnification (Clause 8):
- Issue: Overly broad. We indemnify vendor against “any and all claims” arising from our use.
- Risk: Could hold us liable even for vendor’s negligence or product defects.
- Deviation: Our standard limits indemnity to claims arising from our violation of agreement.

2. Limitation of Liability (Clause 9):
- Issue: Caps all damages at $500, including direct damages.
- Risk: Grossly inadequate for enterprise use. A critical failure could cost millions.
- Deviation: Our standard allows for direct damages up to 12 months of fees paid.

3. IP Ownership (Clause 12):
- Issue: Grants vendor joint ownership of our modifications/integrations.
- Risk: Vendor could use or sell our proprietary algorithms or data insights.
- Deviation: Our standard retains full ownership of our work.

4. Unusual Terms:
- Clause 5.3: Vendor can use our usage data for “any purpose.”
- Clause 7.2: Vendor can terminate for “convenience” with only 7 days’ notice.
- Clause 11: No warranties of any kind, even for basic merchantability.
</findings>

<recommendations>
1. Reject this agreement. Risks far outweigh benefits for an enterprise.
2. Counter-propose:
- Indemnification: Limit to claims arising from our violation of agreement.
- Liability: Cap at 12 months’ fees for direct damages; negotiate for indirect.
- IP: Full ownership of our modifications/integrations.
- Data Use: Restrict to service improvement only.
- Termination: Minimum 90 days’ notice; only for cause.
- Warranties: Demand basic merchantability and fitness for purpose.
3. If vendor refuses, explore alternative providers or in-house solutions.
4. Engage legal counsel for contract negotiation given high stakes.
</recommendations>
```
          - Give Claude a role with a system prompt.
            - When using Claude, you can dramatically improve its performance by using the `system` parameter to give it a role. This technique, known as role prompting, is the most powerful way to use system prompts with Claude. 
            - The right role can turn Claude from a general assistant into your virtual domain expert!
            -  Use the `system` parameter to set Claude’s role. Put everything else, like task-specific instructions, in the `user` turn instead.
            - Why use role prompting?
              - Enhanced accuracy: In complex scenarios like legal analysis or financial modeling, role prompting can significantly boost Claude’s performance.
              - Tailored tone: Whether you need a CFO’s brevity or a copywriter’s flair, role prompting adjusts Claude’s communication style.
              - Improved focus: By setting the role context, Claude stays more within the bounds of your task’s specific requirements.
              - Example: API code example for making Claude a data scientist
                - ```
Experiment with roles! A data scientist might see different insights than a marketing strategist for the same data. A data scientist specializing in customer insight analysis for Fortune 500 companies might yield different results still!
```
                - Experiment with roles! A `data scientist` might see different insights than a `marketing strategist` for the same data. A `data scientist specializing in customer insight analysis for Fortune 500 companies` might yield different results still!
            - Example: Legal contract analysis
              - Without a role, Claude might miss critical issues. With a role, Claude catches critical issues that could cost millions.
              - Role: User
                - Without a Role:
                  - ```
Analyze this software licensing agreement for potential risks:
<contract>
{{CONTRACT}}
</contract>

```
                - With a Role:
                  - ```
You are the General Counsel of a Fortune 500 tech company. We’re considering this software licensing agreement for our core data infrastructure:
<contract>
{{CONTRACT}}
</contract>

Analyze it for potential risks, focusing on indemnification, liability, and IP ownership. Give your professional opinion.

```
              - Role: Assistant
                - Without a Role:
                  - ```
Here’s a summary of the key points in the software licensing agreement:

1. Indemnification (Section 8):
- We indemnify the vendor against claims arising from our use.

2. Limitation of Liability (Section 9):
- Damages capped at $500.

3. IP Ownership (Section 12):
- We retain ownership of our modifications.

Overall, the agreement seems standard. The indemnification and liability clauses are typical, and we maintain our IP rights.
```
                - With a Role:
                  - ```
I’ve reviewed the software licensing agreement, and I have serious concerns that could expose our company to significant risks:

1. Indemnification (Section 8):
- Critical Issue: We indemnify the vendor against “any and all claims” arising from our use.
- Risk: This could hold us liable even for the vendor’s negligence or product defects.
- Recommendation: Reject. Limit our indemnity to claims arising from our violation of the agreement.

2. Limitation of Liability (Section 9):
- Critical Issue: Damages capped at $500, including direct damages.
- Risk: Grossly inadequate. A critical failure in our data infrastructure could cost us millions in lost revenue and reputational damage.
- Recommendation: Reject. Negotiate for cap at 12 months of fees for direct damages, and carve-outs for data breaches.

3. IP Ownership (Section 12):
- Critical Issue: Vendor gains joint ownership of our modifications and integrations.
- Risk: They could use or sell our proprietary algorithms or data insights to competitors.
- Recommendation: Reject. We must retain full ownership of our work.

Professional Opinion: Do not sign this agreement. It is grossly unfavorable and exposes us to unacceptable levels of risk. If the vendor refuses to renegotiate these terms, I strongly advise exploring alternative providers or accelerating our in-house solution. The potential costs of proceeding with this agreement far outweigh any short-term gains.
```
            - Example: Financial analysis
              - Without a role, Claude’s analysis lacks depth. With a role, Claude delivers actionable insights.
              - Role: User
                - Without a Role:
                  - ```
Analyze this dataset of our Q2 financials:
<data>
{{FINANCIALS}}
</data>

Highlight key trends and recommend actions.
```
                - With a Role:
                  - ```
You are the CFO of a high-growth B2B SaaS company. We’re in a board meeting discussing our Q2 financials:
<data>
{{FINANCIALS}}
</data>

Analyze key trends, flag concerns, and recommend strategic actions. Our investors want aggressive growth but are wary of our burn rate.
```
              - Role: Assistant
                - Without a Role:
                  - ```
Here’s a summary of your Q2 financials:

1. Revenue: $15.2M, up 22% YoY. Enterprise client base grew 30%.
2. Gross Margin: 72%, up 3% from Q1.
3. EBITDA Margin: 18%, down 2% due to R&D investments.
4. Cash Flow: $4.1M operating, $28M in reserves.

Overall, financials look strong with revenue growth and healthy margins. Increased R&D spending is impacting profitability but could pay off long-term. Maintain current course.
```
                - With a Role:
                  - ```
Board members, here’s my analysis of our Q2 financials:

1. Revenue ($15.2M, +22% YoY):
- Insight: Enterprise growth (+30%) is outpacing overall growth.
- Flag: SMB segment declined 5%.
- Action: Reallocate 20% of SMB marketing budget to enterprise sales. Task product to simplify SMB onboarding.

2. Margins (Gross: 72%, +3% QoQ; EBITDA: 18%, -2% QoQ):
- Insight: Infrastructure optimizations boosted gross margins.
- Flag: R&D spend on AI features dented EBITDA.
- Action: No change. Our AI roadmap is critical for enterprise stickiness.

3. Cash ($4.1M OCF, $28M reserves):
- Insight: 15-month runway at current burn.
- Flag: CAC up 20%, eroding LTV/CAC.
- Action: Freeze non-essential hires. Implement sales funnel analytics to cut CAC by 15% in Q3.

Strategic Stance: We’re threading the needle between growth and burn. Enterprise focus is our growth engine—invest there. But watch SMB churn; it could bleed into enterprise. Our AI bet is sound, but we must convert it to ARR by Q4, or we cut R&D by 30%.

In sum: Double down on enterprise, streamline SMB, optimize sales, and monetize AI. Questions?
```
          - Chain complex prompts for stronger performance.
            - When working with complex tasks, Claude can sometimes drop the ball if you try to handle everything in a single prompt. Chain of thought (CoT) prompting is great, but what if your task has multiple distinct steps that each require in-depth thought? Enter prompt chaining: breaking down complex tasks into smaller, manageable subtasks.
            - Why chain prompts?
              - Accuracy: Each subtask gets Claude’s full attention, reducing errors.
              - Clarity: Simpler subtasks mean clearer instructions and outputs.
              - Traceability: Easily pinpoint and fix issues in your prompt chain.
            - When to chain prompts?
              - Use prompt chaining for multi-step tasks like research synthesis, document analysis, or iterative content creation. When a task involves multiple transformations, citations, or instructions, chaining prevents Claude from dropping or mishandling steps.
              - Remember: Each link in the chain gets Claude’s full attention!
              - Debugging tip: If Claude misses a step or performs poorly, isolate that step in its own prompt. This lets you fine-tune problematic steps without redoing the entire task.
            - How to chain prompts:
              - Identify subtasks: Break your task into distinct, sequential steps.
              - Structure with XML for clear handoffs: Use XML tags to pass outputs between prompts.
              - Have a single-task goal: Each subtask should have a single, clear objective.
              - Iterate: Refine subtasks based on Claude’s performance.
              - Example chain workflows:
                - Multi-step analysis: See the legal and business examples below.
                - Content creation pipelines: Research → Outline → Draft → Edit → Format.
                - Data processing: Extract → Transform → Analyze → Visualize.
                - Decision-making: Gather info → List options → Analyze each → Recommend.
                - Verification loops: Generate content → Review → Refine → Re-review.
                - Optimization tip: For tasks with independent subtasks (like analyzing multiple docs), create separate prompts and run them in parallel for speed.
            - Example: Self-correction chains
              - You can chain prompts to have Claude review its own work! This catches errors and refines outputs, especially for high-stakes tasks.
              - Prompt 1:
                - Role: User
                  - Content:
                    - ```
Summarize this medical research paper.

<paper>{{RESEARCH_PAPER}}</paper>

Focus on methodology, findings, and clinical implications.
```
                - Role: Assistant (`{{SUMMARY}}` )
                  - Content:
                    - ```
Methodology: Double-blind RCT with 500 Type 2 diabetes patients. Group A (n=250) received novel GLP-1/GIP dual agonist; Group B (n=250) received standard GLP-1 agonist. 24-week trial, primary endpoint: HbA1c reduction.

Findings:
- Group A: 1.8% mean HbA1c reduction
- Group B: 1.1% mean HbA1c reduction
- 68% of Group A vs. 52% of Group B achieved HbA1c <7%
- Group A: 5.2kg mean weight loss
- Group B: 3.1kg mean weight loss

Safety: Similar adverse event profiles, no serious events attributed to treatment.

Clinical Implications: Dual agonist shows superior glycemic control and weight loss compared to standard therapy. Could be a game-changer, especially for obese diabetics.
```
              - Prompt 2:
                - Role: User
                  - Content:
                    - ```
Your task is to provide feedback on a research paper summary. Here is a summary of a medical research paper:
<summary>
{{SUMMARY}}
</summary>

Here is the research paper:
<paper>
{{RESEARCH_PAPER}}
</paper>

Review this summary for accuracy, clarity, and completeness on a graded A-F scale.
```
                - Role: Assistant (`{{FEEDBACK}}` )
                  - Content:
                    - ```
1. Accuracy: A
- Correctly reported key stats: HbA1c reductions, weight loss.
- Accurately noted safety profile and lack of serious adverse events.
- Assertion of “game-changer” is justified by significantly better outcomes.

2. Clarity: A-
- Concise and well-structured with clear sections.
- Good use of comparative language (e.g., “up 3% from Q1”) for context.
- Minor issue: Acronyms like “RCT,” “GLP-1,” and “GIP” not defined. Consider spelling out first use.

3. Completeness: B+
- Covers all requested areas: methodology, findings, clinical implications.
- Missing: Baseline characteristics of groups (were they well-matched?).
- Missing: Any secondary endpoints (e.g., blood pressure, lipids).
- Consider: Long-term implications or need for further research.

Overall, the summary is accurate and clear, providing key insights for a quick grasp of the study. To elevate to A+, define acronyms, mention group baseline stats, note any secondary outcomes, and briefly touch on the need for long-term studies.
```
              - Prompt 3:
                - Role: User
                  - Content:
                    - ```
Your task is to improve a paper summary given feedback. Here is the first draft of a medical research paper:
<summary>
{{SUMMARY}}
</summary>

Here is the research paper:
<paper>
{{RESEARCH_PAPER}}
</paper>

Here is the feedback:
<feedback>
{{FEEDBACK}}
</feedback>

Update the summary based on the feedback.

```
                - Role: Assistant
                  - Content:
                    - ```
Methodology: Double-blind randomized controlled trial (RCT) with 500 Type 2 diabetes patients. Group A (n=250) received a novel glucagon-like peptide-1/glucose-dependent insulinotropic polypeptide (GLP-1/GIP) dual agonist; Group B (n=250) received a standard GLP-1 agonist. The 24-week trial’s primary endpoint was HbA1c reduction. Groups were well-matched for age, BMI, and baseline HbA1c.

Findings:
- Group A: 1.8% mean HbA1c reduction
- Group B: 1.1% mean HbA1c reduction
- 68% of Group A vs. 52% of Group B achieved HbA1c <7%
- Group A: 5.2kg mean weight loss
- Group B: 3.1kg mean weight loss

Secondary endpoints showed improvements in blood pressure and lipid profiles for both groups, with Group A showing slightly better results.

Safety: Similar adverse event profiles, with no serious events attributed to either treatment.

Clinical Implications: The dual GLP-1/GIP agonist demonstrates superior glycemic control and weight loss compared to standard GLP-1 therapy. These results suggest it could be a significant advancement, particularly for obese diabetics. However, longer-term studies are needed to confirm sustained efficacy and safety, as well as to investigate potential benefits on cardiovascular outcomes.

```
            - Example: Analyzing a legal contract (without chaining)
              - Note that Claude misses the instruction to provide proposed changes in its email draft.
              - Role: User
                - Content:
                  - ```
You’re our Chief Legal Officer. Review this SaaS contract for risks, focusing on data privacy, SLAs, and liability caps.

<contract>
{{CONTRACT}}
</contract>

Then draft an email to the vendor with your concerns and proposed changes.
```
              - Role: Assistant
                - Content:
                  - ```
After reviewing the SaaS contract, I’ve identified several concerns:

1. Data Privacy (Section 7):
- Issue: Vendor has broad rights to use client data for “service improvement.”
- Risk: Could lead to unauthorized data sharing or monetization.

2. SLAs (Section 5):
- Issue: 99.5% uptime guarantee is below industry standard.
- Risk: Excessive downtime could impact our operations.

3. Liability (Section 9):
- Issue: Cap set at 3 months of fees.
- Risk: Inadequate for potential data breaches or service failures.

Draft email:

Dear {{VENDOR}},

After reviewing the SaaS agreement, we have concerns:

1. Data Privacy: We require that our data only be used for direct service provision, not broad “service improvement.” Please amend Section 7.

2. SLAs: 99.5% uptime is insufficient. We need 99.99% with clearly defined penalties. Please revise Section 5.

3. Liability: The current cap of 3 months’ fees is inadequate. We propose a cap of 12 months’ fees, with no cap for data breaches or willful misconduct.

We value our potential partnership and believe these changes will set us up for success. Please send a revised agreement.

Best regards,
{{CLO_NAME}}
```
            - Example: Analyzing a legal contract (with chaining)
              - Prompt 1:
                - Role: User
                  - Content:
                    - ```
You’re our Chief Legal Officer. Review this SaaS contract for risks, focusing on data privacy, SLAs, and liability caps.

<contract>
{{CONTRACT}}
</contract>

Output your findings in <risks> tags.

```
                - Role: Assistant (`{{CONCERNS}}` )
                  - Content:
                    - ```
1. Data Privacy (Section 7):
- Issue: Vendor has broad rights to use client data for “service improvement.”
- Risk: Could lead to unauthorized data sharing or monetization of client data.
- Impact: Potential breach of client trust, regulatory fines (GDPR, CCPA).

2. SLAs (Section 5):
- Issue: 99.5% uptime guarantee is below industry standard (typically 99.99%).
- Risk: Up to 43.8 hours of downtime per year vs. 52.6 minutes at 99.99%.
- Impact: Operational disruptions, revenue loss, client dissatisfaction.

3. Liability (Section 9):
- Issue: Liability cap set at 3 months of fees.
- Risk: Inadequate for potential data breaches, IP theft, or prolonged service failures.
- Impact: Insufficient compensation for damages that could far exceed the cap, like class-action lawsuits or regulatory fines.

```
              - Prompt 2:
                - Role: User
                  - Content:
                    - ```
Draft an email to a SaaS product vendor outlining the following concerns and proposing changes. Here are the concerns:
<concerns>
{{CONCERNS}}
</concerns>
```
                - Role: Assistant (`{{EMAIL}}` )
                  - Content:
                    - ```
Dear {{VENDOR}},

After a thorough review of the proposed SaaS agreement, our legal team has identified several high-priority concerns that must be addressed before we can proceed:

1. Data Privacy (Section 7):
- Current: Broad rights to use client data for “service improvement.”
- Concern: Potential unauthorized data sharing or monetization.
- Proposed Change: Limit data use to direct service provision. Any use for improvement, analytics, or marketing must be opt-in, revocable, and anonymized.

2. SLAs (Section 5):
- Current: 99.5% uptime guarantee.
- Concern: Up to 43.8 hours of downtime per year, risking operational disruptions.
- Proposed Change: 99.99% uptime with tiered service credits. E.g., 10% credit for <99.99%, 25% for <99.9%, 50% for <99.5%.

3. Liability (Section 9):
- Current: Cap at 3 months of fees.
- Concern: Grossly inadequate for data breaches, IP theft, or prolonged outages.
- Proposed Change: 12 months’ fees for general liability. Uncapped for data breaches, willful misconduct, IP infringement.

These changes are non-negotiable for us. They align with industry standards and our commitment to data security, operational excellence, and fiduciary responsibility. We value the potential of our partnership, and addressing these will set a strong foundation.

Please have your legal team review and incorporate these changes. We’re happy to discuss further to ensure mutual understanding and agreement.

Best regards,
{{CLO_NAME}}
```
              - Prompt 3:
                - Role: User
                  - Content:
                    - ```
Your task is to review an email and provide feedback. Here is the email:
<email>
{{EMAIL}}
</email>

Give feedback on tone, clarity, and professionalism.
```
                - Role: Assistant
                  - Content:
                    - ```
1. Tone:
- Professional and assertive without being confrontational.
- Use of “non-negotiable” is strong but justified given the criticality of the issues.
- Collaborative language (“we value,” “happy to discuss”) balances the firmness, maintaining a constructive tone.

2. Clarity:
- Excellent structuring: Current state, concern, and proposed change for each issue.
- Technical terms (e.g., “EBITDA,” “tiered service credits”) are appropriate for a B2B SaaS context.
- Action items are clear: vendor’s legal team to review and incorporate changes.

3. Professionalism:
- Focuses on mutual benefit and industry standards, not just our demands.
- Mentions broader responsibilities (data security, operational excellence, fiduciary duty), elevating the discussion.
- Offers further discussion, showing commitment to understanding and resolving issues.

Overall, the email strikes the right balance of assertiveness and collaboration. It clearly articulates our concerns, proposes specific solutions, and maintains a professional, partnership-oriented tone. No revisions needed; proceed with sending.
```
            - Example: Multi-tenancy strategy review
              - Prompt 1:
                - Role: User
                  - Content:
                    - ```
As a senior solutions architect, review and analyze this multitenancy strategy for our new enterprise SaaS product.

<strategy>
{{STRATEGY}}
</strategy>

Focus on scalability, security, and cost-effectiveness.
```
                - Role: Assistant (`{{ANALYSIS}}` )
                  - Content:
                    - ```
1. Scalability:
+ Database sharding by tenant reduces contention, supports high growth.
+ Microservices allow independent scaling of components.
- Shared app servers could bottleneck under extreme load.
- No mention of auto-scaling or load balancing for app tier.

2. Security:
+ Tenant data isolation via sharding mitigates data leaks.
+ JWT with short expiry and refresh tokens is industry best practice.
- No mention of encryption (at rest, in transit) or key management.
- IAM strategy lacks detail on roles, permissions, or RBAC.

3. Cost-effectiveness:
+ Shared app servers reduce idle resource costs.
+ Serverless functions for infrequent tasks optimize costs.
- Per-tenant DB could increase costs at scale vs. optimized multi-tenant DB.
- No discussion of reserved instances or spot instances for cost saving.
```
              - Prompt 2:
                - Role: User
                  - Content:
                    - ```
Draft a strategy review document for engineering leadership based on this analysis of a multitenancy strategy.

<strategy>
{{STRATEGY}}
</strategy>

<analysis>
{{ANALYSIS}}
</analysis>

Include an executive summary, detailed analysis, and recommendations.

```
                - Role: Assistant (`{{STRATEGY_DOC}}` )
                  - Content:
                    - ```
# Multitenancy Strategy Review: SaaS Enterprise Product

## Executive Summary
The proposed strategy demonstrates a solid foundation in scalability and security principles but has gaps in implementation details. It excels in data isolation and component-level scaling but lacks robust app-tier scaling and advanced security measures. Cost optimizations are present but may falter at high scale. With targeted enhancements, this strategy can effectively support our enterprise SaaS ambitions.

## Detailed Analysis

### 1. Scalability: B+

Strengths:
- Database sharding by tenant is excellent. It minimizes query latency, supports high data growth, and facilitates seamless per-tenant scaling.
- Microservices architecture allows scaling individual components based on demand, crucial for handling diverse enterprise workloads.

Weaknesses:
- Shared application servers risk becoming a bottleneck. As tenant count and traffic grow, these could degrade performance across all tenants.
- Absence of auto-scaling and load balancing for the app tier is a critical oversight. These are essential for handling traffic spikes without over-provisioning.

### 2. Security: B-

Strengths:
- Sharded databases provide strong tenant data isolation, mitigating risks of data leaks or unauthorized access between tenants.
- JWT with short expiry and refresh tokens align with OWASP best practices, reducing token vulnerability windows.

Weaknesses:
- No mention of data encryption at rest or in transit (TLS 1.3). This is non-negotiable for enterprise data.
- Key management strategy is missing. How are encryption keys, JWT secrets, and DB credentials managed and rotated?
- IAM lacks depth. We need RBAC with well-defined roles, attribute-based access control (ABAC) for fine-grained permissions, and integration with enterprise identity providers (SAML, OIDC).

### 3. Cost-effectiveness: B

Strengths:
- Shared app servers reduce idle capacity costs, good for steady-state traffic.
- Serverless for infrequent tasks (e.g., reporting) is smart; pay only when these resources are used.

Weaknesses:
- Per-tenant databases may not be cost-effective at scale compared to a well-designed multi-tenant database with robust data partitioning.
- No mention of cost-saving measures like reserved instances for predictable workloads or spot instances for fault-tolerant tasks.
- Lack of app-tier auto-scaling could lead to over-provisioning, increasing costs.

## Recommendations

1. App Tier Resilience: Implement auto-scaling with Kubernetes or cloud-native services (EKS, GKE). Deploy across availability zones with a CDN for static assets.

2. Security Hardening:
a. Encrypt data at rest (AES-256) and in transit (TLS 1.3).
b. Use a managed KMS (AWS KMS, GCP KMS) for key management.
c. Enhance IAM with RBAC, ABAC, and SAML/OIDC for enterprise SSO.

3. Cost Optimization:
a. Evaluate columnar multi-tenant DB (e.g., Snowflake) vs. sharding at scale.
b. Use reserved instances for app servers, spot instances for non-critical workloads.
c. Implement proactive monitoring and right-sizing to avoid overprovisioning.

4. Proof of Concept (PoC): Before full implementation, conduct a PoC with a high-scale simulation:
a. Test sharded DB vs. optimized multi-tenant DB for cost and performance.
b. Stress-test app tier with simulated enterprise-level traffic spikes.
c. Conduct third-party security audit, focusing on data isolation and access controls.

With these enhancements, our multi-tenancy strategy will be enterprise-grade: scalable, secure, and cost-effective. The PoC will validate our design choices and instill confidence in our enterprise clients.
```
              - Prompt 3:
                - Role: User
                  - Content:
                    - ```
Grade this strategy review document for clarity, actionability, and alignment with enterprise priorities.

<priorities>
{{PRIORITIES}}
</priorities>

<strategy_doc>
{{STRATEGY_DOC}}
</strategy_doc>

```
                - Role: Assistant
                  - Content:
                    - ```
1. Clarity: A
+ Clear structure with Executive Summary, Detailed Analysis, and Recommendations.
+ Analysis uses a grading system (B+, B-) for quick grasp of strengths/weaknesses.
+ Technical terms (RBAC, ABAC, TLS) are explained or contextualized, aiding non-technical leadership.
+ Visual aids (bullet points, indentation) enhance readability.

2. Actionability: A-
+ Recommendations are specific and technical (e.g., “Implement auto-scaling with Kubernetes”).
+ PoC plan with clear objectives provides a roadmap.
+ Missing: Timelines, resource requirements, or team assignments for each recommendation.
+ Consider: Add a “Next Steps” section with immediate actions, roles, and a high-level timeline.

3. Enterprise Alignment: A+
+ Directly addresses enterprise priorities:
- Scalability: Discusses handling enterprise-level traffic spikes.
- Security: Focuses on data isolation, encryption, and enterprise IAM.
- Cost: Strategies align with enterprise FinOps (reserved instances, right-sizing).
+ PoC with “high-scale simulation” and security audit shows commitment to enterprise-grade quality.
+ Language resonates with enterprise concerns: “unacceptable risk,” “enterprise-grade,” “instill confidence.”

Overall, the document excels in clarity and enterprise alignment. The actionability is strong but could be elevated to A+ with a “Next Steps” section. The technical depth, coupled with clear explanations, makes this highly effective for both technical and non-technical leadership. Great work!
```
          - Long context prompting tips.
            - Claude’s extended context window (200K tokens for Claude 3 models) enables handling complex, data-rich tasks. This guide will help you leverage this power effectively.
            - Essential tips for long context prompts:
              - Put longform data at the top: Place your long documents and inputs (~20K+ tokens) near the top of your prompt, above your query, instructions, and examples. This can significantly improve Claude’s performance across all models.
                - Queries at the end can improve response quality by up to 30% in tests, especially with complex, multi-document inputs.
              - Structure document content and metadata with XML tags: When using multiple documents, wrap each document in `<document>` tags with `<document_content>` and `<source>` (and other metadata) subtags for clarity.
                - Example multi-document structure:
                  - ```
<documents>
  <document index="1">
    <source>annual_report_2023.pdf</source>
    <document_content>
      {{ANNUAL_REPORT}}
    </document_content>
  </document>
  <document index="2">
    <source>competitor_analysis_q2.xlsx</source>
    <document_content>
      {{COMPETITOR_ANALYSIS}}
    </document_content>
  </document>
</documents>

Analyze the annual report and competitor analysis. Identify strategic advantages and recommend Q3 focus areas.
```
              - Ground responses in quotes: For long document tasks, ask Claude to quote relevant parts of the documents first before carrying out its task. This helps Claude cut through the “noise” of the rest of the document’s contents.
                - Example quote extraction:
                  - ```
You are an AI physician's assistant. Your task is to help doctors diagnose possible patient illnesses.

<documents>
  <document index="1">
    <source>patient_symptoms.txt</source>
    <document_content>
      {{PATIENT_SYMPTOMS}}
    </document_content>
  </document>
  <document index="2">
    <source>patient_records.txt</source>
    <document_content>
      {{PATIENT_RECORDS}}
    </document_content>
  </document>
  <document index="3">
    <source>patient01_appt_history.txt</source>
    <document_content>
      {{PATIENT01_APPOINTMENT_HISTORY}}
    </document_content>
  </document>
</documents>

Find quotes from the patient records and appointment history that are relevant to diagnosing the patient's reported symptoms. Place these in <quotes> tags. Then, based on these quotes, list all information that would help the doctor diagnose the patient's symptoms. Place your diagnostic information in <info> tags.
```
          - Extended thinking tips.
            - Extended thinking allows Claude to work through complex problems step-by-step, improving performance on difficult tasks.
            - Technical considerations for extended thinking:
              - Thinking tokens have a minimum budget of 1024 tokens. We recommend that you start with the minimum thinking budget and incrementally increase to adjust based on your needs and task complexity.
              - For workloads where the optimal thinking budget is above 32K, we recommend that you use [batch processing](https://docs.anthropic.com/en/docs/build-with-claude/batch-processing) to avoid networking issues. Requests pushing the model to think above 32K tokens causes long running requests that might run up against system timeouts and open connection limits.
              - Extended thinking performs best in English, though final outputs can be in [any language Claude supports](https://docs.anthropic.com/en/docs/build-with-claude/multilingual-support).
              - If you need thinking below the minimum budget, we recommend using standard mode, with thinking turned off, with traditional chain-of-thought prompting with XML tags (like `<thinking>`). See [chain of thought prompting](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought).
            - Prompting techniques for extended thinking:
              - Use general instructions first, then troubleshoot with more step-by-step instructions
                - Claude often performs better with high level instructions to just think deeply about a task rather than step-by-step prescriptive guidance. The model’s creativity in approaching problems may exceed a human’s ability to prescribe the optimal thinking process.
                - Example:
                  - Instead of:
                    - ```
Think through this math problem step by step: 
1. First, identify the variables
2. Then, set up the equation
3. Next, solve for x
...
```
                  - Consider:
                    - ```
Please think about this math problem thoroughly and in great detail. 
Consider multiple approaches and show your complete reasoning.
Try different methods if your first approach doesn't work.
```
              - Multi-shot prompting with extended thinking
                - [Multishot prompting](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting) works well with extended thinking. When you provide Claude examples of how to think through problems, it will follow similar reasoning patterns within its extended thinking blocks.
                - You can include few-shot examples in your prompt in extended thinking scenarios by using XML tags like `<thinking>`  or `<scratchpad>`  to indicate canonical patterns of extended thinking in those examples.
                - Claude will generalize the pattern to the formal extended thinking process. However, it’s possible you’ll get better results by giving Claude free rein to think in the way it deems best.
                - Example:
                  - ```
I'm going to show you how to solve a math problem, then I want you to solve a similar one.

Problem 1: What is 15% of 80?

<thinking>
To find 15% of 80:
1. Convert 15% to a decimal: 15% = 0.15
2. Multiply: 0.15 × 80 = 12
</thinking>

The answer is 12.

Now solve this one:
Problem 2: What is 35% of 240?
```
              - Maximize instruction following with extending thinking
                - Claude shows significantly improved instruction following when extended thinking is enabled. The model typically:
                  - Reasons about instructions inside the extended thinking block
                  - Executes those instructions in the response
                - To maximize instruction following:
                  - Be clear and specific about what you want
                  - For complex instructions, consider breaking them into numbered steps that Claude should work through methodically
                  - Allow Claude enough budget to process the instructions fully in its extended thinking
              - Using extended thinking to debug and steer Claude's behavior
                - You can use Claude’s thinking output to debug Claude’s logic, although this method is not always perfectly reliable.
                - To make the best use of this methodology, we recommend the following tips:
                  - We don’t recommend passing Claude’s extended thinking back in the user text block, as this doesn’t improve performance and may actually degrade results.
                  - Prefilling extended thinking is explicitly not allowed, and manually changing the model’s output text that follows its thinking block is likely going to degrade results due to model confusion.
                - When extended thinking is turned off, standard `assistant` response text [prefill](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response) is still allowed.
                - Sometimes Claude may repeat its extended thinking in the assistant output text. If you want a clean response, instruct Claude not to repeat its extended thinking and to only output the answer.
              - Making the best of long outputs and longform thinking
                - For dataset generation use cases, try prompts such as “Please create an extremely detailed table of…” for generating comprehensive datasets.
                - For use cases such as detailed content generation where you may want to generate longer extended thinking blocks and more detailed responses, try these tips:
                  - Increase both the maximum extended thinking length AND explicitly ask for longer outputs
                  - For very long outputs (20,000+ words), request a detailed outline with word counts down to the paragraph level. Then ask Claude to index its paragraphs to the outline and maintain the specified word counts
                - We do not recommend that you push Claude to output more tokens for outputting tokens’ sake. Rather, we encourage you to start with a small thinking budget and increase as needed to find the optimal settings for your use case.
              - Have Claude reflect on and check its work for improved consistency and error handling
                - You can use simple natural language prompting to improve consistency and reduce errors:
                  - Ask Claude to verify its work with a simple test before declaring a task complete
                  - Instruct the model to analyze whether its previous step achieved the expected result
                  - For coding tasks, ask Claude to run through test cases in its extended thinking
                - Example:
                  - ```
Write a function to calculate the factorial of a number. 
Before you finish, please verify your solution with test cases for:
- n=0
- n=1
- n=5
- n=10
And fix any issues you find.
```
      - Lakera AI — “10 Techniques for Effective Prompt Engineering”
        - Summary:
          - Covers system prompts, chain‑of‑thought, role play, and guardrails.
          - Provides security‑oriented tips (sanitizing user input, avoiding prompt injection).
        - Link:
          - [https://www.lakera.ai/blog/prompt-engineering-guide](https://www.lakera.ai/blog/prompt-engineering-guide)
    - Insights on Category 1:
      - Clear, explicit instructions paired with structure (tags, JSON schemas) consistently cut hallucination and formatting errors.
      - Step‑by‑step or chain‑of‑thought cues boost reasoning accuracy but may expose internal reasoning; hide or post‑process in production.
  - Category 2: Courses & Guides for Learning Prompt Engineering
    - Summary:
      - Educational resources—both interactive courses and living wikis—that teach developers and teams how to master prompt design quickly.
    - Sources
      - DeepLearning.AI — “ChatGPT Prompt Engineering for Developers”
        - Summary:
          - 1.5‑hour free course co‑taught by Andrew Ng & Isa Fulford.
          - Demonstrates two core prompting principles and multiple real‑world tasks using OpenAI API.
        - Link:
          - [https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
      - DAIR.AI — “Prompt Engineering Guide”
        - Summary:
          - Massive open‑source repository of prompt patterns, research papers, and examples.
          - Updated frequently, acting as a hub for the latest techniques.
        - Link:
          - [https://dair.ai/projects/prompt-engineering/](https://dair.ai/projects/prompt-engineering/)
      - [LearnPrompting.org](http://learnprompting.org/)
        - Summary:
          - Community‑run interactive curriculum with beginner to advanced modules.
          - Includes prompt hacking and retrieval‑augmented generation lessons.
        - Link:
          - [https://learnprompting.org](https://learnprompting.org/)
    - Insights on Category 2:
      - Short, hands‑on courses accelerate onboarding, while evergreen guides serve as reference for edge cases and new research.
      - Community‑maintained wikis crowd‑source emerging techniques faster than formal publications.
  - Category 3: Tools & Frameworks for Prompt‑Driven Workflows
    - Summary:
      - Open‑source libraries and repos that help structure, version, and evaluate prompts at scale.
    - Sources
      - OpenAI Cookbook (GitHub)
        - Summary:
          - Jupyter notebooks with prompt templates, eval harnesses, and function calling examples.
          - “GPT prompting guide” shows how minor wording tweaks shift GPT‑4 output fidelity.
        - Link:
          - [https://github.com/openai/openai-cookbook](https://github.com/openai/openai-cookbook)
      - LangChain — Prompt Templates Docs
        - Summary:
          - Provides templating, variable interpolation, and chaining utilities to compose multi‑step prompts in code.
          - Supports memory and tool integrations for complex agents.
        - Link:
          - [https://python.langchain.com/docs/concepts/prompt_templates/](https://python.langchain.com/docs/concepts/prompt_templates/)
      - Guidance (Microsoft) — GitHub
        - Summary:
          - DSL for interleaving generation, control flow, and constraints in a single prompt program.
          - Enables regex/CFG‑based output enforcement and reduces token usage.
        - Link:
          - [https://github.com/guidance-ai/guidance](https://github.com/guidance-ai/guidance)
    - Insights on Category 3:
      - Treat prompts as code: templates, version control, and tests dramatically reduce regression risk when models update.
      - Libraries that mix control flow with prompting (e.g., Guidance) unlock deterministic, parseable outputs ideal for production APIs.
  - Category 4: Chain‑of‑Thought Variants for Deep Reasoning
    - Summary:
      - Methods that extend basic CoT by sampling multiple chains, exploring trees/graphs of thoughts, or decomposing problems incrementally.
    - Sources
      - Wei et al. — “Chain‑of‑Thought Prompting Reveals Reasoning in LLMs”
        - Summary:
          - Introduced adding "Let's think step by step" to elicit stepwise reasoning.
          - Showed big accuracy gains on arithmetic and commonsense tasks.
        - Link:
          - [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
      - Wang et al. — “Self‑Consistency Improves Chain‑of‑Thought Reasoning”
        - Summary:
          - Generates many CoT samples with high temperature, then majority‑votes answers.
          - Boosted GSM8K math accuracy from 57% → 74% using GPT‑3.
        - Link:
          - [https://arxiv.org/abs/2203.11171](https://arxiv.org/abs/2203.11171)
      - Yao et al. — “Tree of Thoughts: Deliberate Reasoning via Guided Tree Search”
        - Summary:
          - Frames reasoning as a tree search over thought branches with evaluation pruning.
          - Outperformed linear CoT on puzzles and planning tasks by 35‑50%.
        - Link:
          - [https://arxiv.org/abs/2305.10601](https://arxiv.org/abs/2305.10601)
    - Insights on Category 4 (NOT optional):
      - Sampling diverse reasoning paths (self‑consistency) cures single‑chain brittleness and significantly raises accuracy.
      - Structured search (ToT) allows backtracking and look‑ahead, enabling LLMs to crack problems that stump linear reasoning.
  - Category 5: Tool‑Augmented Reasoning (ReAct, PAL, PoT)
    - Summary:
      - Prompts that interleave reasoning with tool calls or code execution, letting LLMs fetch external info or run precise computations.
    - Sources
      - Yao et al. — “ReAct: Synergizing Reasoning and Acting in Language Models”
        - Summary:
          - Introduces Thought/Action loops where LLM chooses actions (e.g., search) based on its reasoning.
          - Improves factual QA and interactive decision making by >20 points on HotpotQA.
        - Link:
          - [https://arxiv.org/abs/2210.03629](https://arxiv.org/abs/2210.03629)
      - Gao et al. — “PAL: Program‑Aided Language Models”
        - Summary:
          - Models write Python programs to solve math/logic tasks, then execute them.
          - Eliminates arithmetic errors; GPT‑3 PAL hit 87% on GSM8K vs 55% vanilla.
        - Link:
          - [https://arxiv.org/abs/2211.10435](https://arxiv.org/abs/2211.10435)
      - Chen et al. — “Program‑of‑Thoughts Prompting”
        - Summary:
          - Mixes natural language reasoning with inline code snippets executed mid‑prompt.
          - Further boosts math problem accuracy over PAL by combining CoT + code.
        - Link:
          - [https://arxiv.org/abs/2305.20050](https://arxiv.org/abs/2305.20050)
    - Insights on Category 5 (NOT optional):
      - Letting models call tools or run code converts fuzzy reasoning into deterministic actions, slashing hallucinations.
      - Combining CoT with code (PoT) merges human‑like reasoning and computer‑level precision for numerical tasks.
  - Category 6: Retrieval‑Augmented Generation for Grounded Answers
    - Summary:
      - Pipelines that prepend retrieved documents to prompts so LLMs can cite fresh, domain‑specific knowledge.
    - Sources
      - Lewis et al. — “Retrieval‑Augmented Generation for Knowledge‑Intensive NLP Tasks”
        - Summary:
          - Pioneered combining dense retrieval with seq‑to‑seq generation.
          - Demonstrated large gains on open‑domain QA and factoid tasks.
        - Link:
          - [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)
      - LlamaIndex Docs — “Building an RAG Pipeline with GPT”
        - Summary:
          - Step‑by‑step tutorial for indexing docs, retrieving chunks, and constructing prompts.
          - Shows how chunk size & ordering affect GPT‑4 answer quality.
        - Link:
          - [https://docs.llamaindex.ai/en/latest/guides/retriever.html](https://docs.llamaindex.ai/en/latest/guides/retriever.html)
      - OpenAI Cookbook — “Knowledge Retrieval Example”
        - Summary:
          - Example notebook embedding docs, querying a vector DB, and feeding context into GPT‑4.
          - Provides eval template to measure grounded‑answer accuracy.
        - Link:
          - [https://github.com/openai/openai-cookbook/tree/main/examples/retrieval](https://github.com/openai/openai-cookbook/tree/main/examples/retrieval)
    - Insights on Category 6 (NOT optional):
      - Supplying relevant passages at inference sharply reduces hallucinations and keeps answers current without model retraining.
      - Clear context delimiters and "answer from context only" instructions maximize grounding fidelity.
  - Category 7: Self‑Correction & Meta‑Prompting
    - Summary:
      - Techniques where LLMs critique their own outputs or generate/refine prompts to boost final response quality.
    - Sources
      - Madaan et al. — “Self‑Refine: Iterative Improvement with LLMs”
        - Summary:
          - Model produces answer → critiques it → revises; loop lifts quality ~20% across 7 tasks.
          - Works best with stronger models like GPT‑4.
        - Link:
          - [https://arxiv.org/abs/2303.17651](https://arxiv.org/abs/2210.01904)
      - Zhou et al. — “Large Language Models Are Instruction Learners” (Automatic Prompt Engineer)
        - Summary:
          - Uses an LLM to search the space of prompts and select ones that maximize validation accuracy.
          - Discovered superior CoT trigger phrases outperforming human prompts.
        - Link:
          - [https://arxiv.org/abs/2210.01904](https://arxiv.org/abs/2210.01904)
      - Shinn et al. — “Reflexion: Language Agents with Self‑Reflection”
        - Summary:
          - Agents store memories, reflect on failures, and retry tasks, achieving higher success in code generation and games.
          - Incorporates self‑feedback within multi‑step agent loops.
        - Link:
          - [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)
    - Insights on Category 7 (NOT optional):
      - Asking a model to be its own critic uncovers errors invisible in the first pass, yielding markedly improved answers with minimal extra cost.
      - Meta‑prompting lets expensive models design optimized prompts that cheaper models can follow, reducing inference bills while keeping quality high.
  - Category 8: Foundations of Prompt Engineering
    - Summary:
      - Effective prompting starts with clarity, context, explicit instructions, and illustrative examples. These fundamentals differentiate prompt engineering from costly finetuning and can be jump‑started with tooling like the Prompt Generator.
    - Sources
      - Prompt engineering overview
        - Summary:
          - Defines when and why to prompt engineer versus finetune.
          - Lists a hierarchy of nine core techniques and stresses measuring success criteria first.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)
        - Insights (optional):
          - Prompt engineering preserves model general knowledge while allowing rapid, transparent iteration.
      - Be clear, direct, and detailed
        - Summary:
          - Provide contextual information, be specific, and give step‑wise instructions.
          - Clear prompts reduce hallucinations; examples show anonymization and marketing email improvement.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct)
      - Use examples (multishot prompting)
        - Summary:
          - 3‑5 diverse, relevant examples dramatically improve accuracy and consistency.
          - Wrap examples in tags to guide structure.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting)
      - Prompt generator
        - Summary:
          - Console/Colab tool that auto‑creates first‑draft prompt templates following best practices.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator)
    - Insights on Category 8:
      - Insight 1: Clarity plus concrete examples forms the bedrock of reliable outputs across all later techniques.
      - Insight 2: Rapid template generation, then refinement with clear and multishot principles, accelerates experimentation while maintaining quality.
  - Category 9: Reasoning & Thinking Strategies
    - Summary:
      - Chain‑of‑thought and extended‑thinking modes encourage Claude to work step‑by‑step, producing deeper, more accurate analyses—especially for complex or multi‑factor problems.
    - Sources
      - Let Claude think (chain of thought prompting)
        - Summary:
          - Prompt with “Think step‑by‑step” or custom tags to expose reasoning.
          - Structured prompts cleanly separate thought process from final answer.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-of-thought)
      - Extended thinking tips
        - Summary:
          - Extended models allocate ≥ 1024 thinking tokens and allow reflection, self‑checks, and multiple approaches.
          - Recommend starting with high‑level instructions, then adding prescriptive steps only where needed.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips)
    - Insights on Category 9:
      - Insight 1: Revealing the chain‑of‑thought aids debugging and prompt iteration, turning reasoning into an inspectable asset.
      - Insight 2: High‑level guidance first taps model creativity; detailed steps can then fine‑tune accuracy without stifling insight.
  - Category 10: Managing Complexity and Context
    - Summary:
      - Break multi‑stage tasks into chained subtasks and structure large document inputs with XML tags; place long content first and ground answers in quoted snippets to minimize hallucinations.
    - Sources
      - Chain complex prompts for stronger performance
        - Summary:
          - Decompose tasks into sequential prompts, improving accuracy, clarity, and traceability.
          - Demonstrates legal review, strategy, and self‑correction chains.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/chain-prompts)
      - Long context prompting tips
        - Summary:
          - Put long documents at the top of the prompt, tag each with metadata, and ask Claude to cite or quote.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/long-context-tips)
    - Insights on Category 10:
      - Insight 1: Chained subtasks keep reasoning focused, while well‑ordered XML‑tagged inputs ensure the right context is retrieved.
      - Insight 2: Quote‑grounding acts as a guardrail, reducing error propagation when handling 100K‑token contexts.
  - Category 11: Output Control and Model Behavior
    - Summary:
      - Claude 4 offers finer control via explicit instructions, XML tags, prefilling, and parallel tool invocation, enabling deterministic formats and efficient workflows.
    - Sources
      - Claude 4 prompt engineering best practices
        - Summary:
          - Stress explicitness, context, example vigilance, XML formatting, and parallel tool calls; includes frontend‑coding guidance.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices)
      - Prefill Claude's response for greater output control
        - Summary:
          - Seed the assistant message to skip preamble, force JSON, or keep roleplay character consistent.
        - Link:
          - [https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response)
    - Insights on Category 11:
      - Insight 1: Prefilling plus explicit formatting rules yields machine‑parsable, zero‑preamble responses ideal for automation.
      - Insight 2: Parallel tool calling multiples the gains from chained prompts, executing independent steps simultaneously for speed.
  - Category 12: Prompt Evaluation, Benchmarking & A/B Testing
    - Summary:
      - Frameworks, metrics, and tools for objectively measuring prompt quality, catching regressions, and running online experiments.
    - Sources
      - OpenAI — "openai/evals" Framework
        - Summary:
          - Python toolkit to define evaluation tasks, run prompts, and record metrics.
          - Ships with registry of benchmarks (GSM8K, MMLU, etc.) and LLM‑as‑judge helpers.
        - Link:
          - [https://github.com/openai/evals](https://github.com/openai/evals)
        - Insights:
          - Enables unit‑test‑style prompt regression tests integrated with CI.
      - Stanford CRFM — "HELM: Holistic Evaluation of Language Models" (Liang et al.)
        - Summary:
          - Proposes multi‑metric, multi‑scenario evaluation (accuracy, robustness, bias, cost, etc.).
          - Public dashboard compares dozens of models under identical prompts.
        - Link:
          - [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/)
      - Perez et al. — "G‑Eval: NLG Evaluation using GPT‑4 as a Judge"
        - Summary:
          - Shows GPT‑4 judgments correlate 0.9 with humans on summarization & QA.
          - Provides prompt template and code for reference‑free evaluation.
        - Link:
          - [https://arxiv.org/abs/2303.16634](https://arxiv.org/abs/2303.16634)
      - TruEra — "TruLens" Docs
        - Summary:
          - Open‑source library with feedback functions (toxicity, factuality, relevance).
          - Dashboards track prompt quality over time; integrates with LangChain.
        - Link:
          - [https://www.trulens.org/](https://www.trulens.org/)
      - HumanLoop Blog — "A/B Testing LLM Prompts in Production"
        - Summary:
          - Step‑by‑step guide to routing traffic between prompt variants, logging metrics, and iterating.
          - Discusses sample size, primary vs guardrail metrics, and canary rollout.
        - Link:
          - [https://humanloop.com/blog/ab-testing-llm-prompts](https://humanloop.com/blog/ab-testing-llm-prompts)
      - Langfuse Docs — "Experimentation & AB Testing"
        - Summary:
          - SDK to tag prompt versions and randomly assign users, with automatic metric capture.
          - UI displays response distribution, token cost, and latency per variant.
        - Link:
          - [https://docs.langfuse.com/experiments](https://docs.langfuse.com/experiments)
      - IBM Research — "Prompt Leakage Risk Assessment" (watsonx.governance)
        - Summary:
          - Automated suite of prompt‑injection attacks with scoring of leakage severity.
          - Helps quantify robustness and guide mitigation.
        - Link:
          - [https://www.ibm.com/docs/en/watsonx-governance](https://www.ibm.com/docs/en/watsonx-governance)
    - Insights on Category 12 (NOT optional):
      - Combining human ratings with LLM‑judge and deterministic checks yields fast yet reliable eval pipelines.
      - Continuous, automated tests (OpenAI Evals, TruLens) act as unit tests preventing silent quality drift after prompt edits.
      - Online A/B testing (Langfuse, HumanLoop) validates offline gains with real user engagement and safety metrics.
  - Category 13: LLMs in Software Development: Applications, Impact, and Perspectives
    - Summary:
      - Explores the practical integration of LLMs, particularly through agent-based systems, into software development workflows. This category covers the impact on developer productivity, common challenges and skepticisms, and the evolving role of engineers in an AI-assisted coding landscape.
    - Sources
      - Ptacek, Thomas — "You're All Nuts" (Fly.io Blog)
        - Summary:
          - A provocative argument that LLM-powered agents, which can directly interact with codebases, run tools, compile, and test, are fundamentally changing software development. 
          - Ptacek asserts these agents effectively handle tedious coding tasks, enhance productivity, and mitigate issues like hallucination through iterative feedback. He challenges common developer skepticism regarding code quality, the 'craft' of programming, and IP concerns, urging an embrace of this paradigm shift where human developers guide and curate AI-assisted work.
        - Link:
          - [https://fly.io/blog/youre-all-nuts/](https://www.google.com/url?sa=E&q=https%3A%2F%2Ffly.io%2Fblog%2Fyoure-all-nuts%2F)
    - Insights on Category 13:
      - LLM agents that actively participate in the development cycle (e.g., compiling, testing, version control interaction) represent a more impactful integration into software engineering than passive code generation alone.
      - Overcoming developer skepticism towards AI-assisted coding often involves understanding that LLMs augment critical human skills (judgment, curation, architectural decisions) rather than fully replacing them, and that robust agent capabilities can mitigate common LLM limitations.
      - The practical utility and adoption rate of LLMs in coding can be significantly influenced by the target programming language's characteristics (e.g., structure, standard library, idiomatic usage) and the maturity of AI-specific tooling within its ecosystem.
  - Category 14: Chain-of-Thought (CoT) Reasoning Prompts
    - Summary:
      - “Think step-by-step” or CoT prompts ask the model to reveal intermediate reasoning, dramatically lifting accuracy on multi-step tasks.
      - 2024 research shows CoT can be surfaced automatically through alternative decoding and now extends to multimodal (text + vision) inputs.
      - Surveys emphasize emerging taxonomies, open datasets, and challenges such as evaluating rationale quality.
    - Sources
      - ACL 2024 paper “Navigate through Enigmatic Labyrinth: A Survey of Chain-of-Thought Reasoning”
        - Summary:
          - Synthesizes 100 + CoT studies; groups methods into manual, self-consistency, least-to-most, etc.
          - Reports ≥30 % reasoning-accuracy gains on math, code, and commonsense benchmarks.
        - Link:
          - [https://aclanthology.org/2024.acl-long.65/](https://aclanthology.org/2024.acl-long.65/)
        - Insights (optional):
          - Calls for automatic metrics to score faithfulness of generated rationales.
      - X. Wang & D. Zhou (2024) “Chain-of-Thought Reasoning Without Prompting”
        - Summary:
          - Shows CoT traces emerge by sampling top-k beams—no special prompt needed.
          - Implies reasoning ability is latent in pre-trained weights, not just prompt-triggered.
        - Link:
          - [https://arxiv.org/abs/2402.10200](https://arxiv.org/abs/2402.10200)
      - SuperAnnotate Blog “Chain-of-Thought Prompting: Complete Overview” (2024)
        - Summary:
          - Walks through zero-shot and few-shot CoT templates; demos 2024 multimodal CoT (image → rationale → answer).
        - Link:
          - [https://www.superannotate.com/blog/chain-of-thought-cot-prompting](https://www.superannotate.com/blog/chain-of-thought-cot-prompting)
    - Insights on Category 14 (NOT optional):
      - Insight 1: The field is moving from explicit “show your work” prompts toward decoding tricks that unlock CoT automatically, reducing engineering overhead.
      - Insight 2: Multimodal CoT indicates a path to unified reasoning prompts where text, vision, and possibly other modalities share a single step-wise rationale format.
  - Category 15: ReAct (Reason + Act) Tool-Augmented Prompting
    - Summary:
      - ReAct interleaves the model’s thoughts with tool calls, letting LLMs fetch external data while reasoning.
      - Widely adopted in 2024 – 25 for web-search agents, coding copilots, and data-analysis assistants.
      - Integrates smoothly with function-calling APIs and frameworks like LangChain Agents.
    - Sources
      - Prompt Engineering Guide “ReAct Prompting”
        - Summary:
          - Provides canonical ReAct template and showcases tasks (QA, games) where reasoning + action beats plain CoT.
        - Link:
          - [https://www.promptingguide.ai/techniques/react](https://www.promptingguide.ai/techniques/react)
      - J. Schrier blog “Implementing the ReAct LLM Agent Pattern the Hard Way and the Easy Way” (Jan 2024)
        - Summary:
          - Walk-through from scratch, then with LangChain Agent API; shows ~40 % latency cut when caching reasoning steps.
        - Link:
          - [https://jschrier.github.io/blog/2024/01/13/Implementing-the-ReAct-LLM-Agent-pattern-the-hard-way-and-the-easy-way.html](https://jschrier.github.io/blog/2024/01/13/Implementing-the-ReAct-LLM-Agent-pattern-the-hard-way-and-the-easy-way.html)
      - Bluetick Consultants “Enhancing LLMs with ReAct and RAG Integration” (2024)
        - Summary:
          - Explains pairing ReAct with Retrieval-Augmented Generation pipelines to ground tool results and citations.
        - Link:
          - [https://www.bluetickconsultants.com/integrating-reasoning-and-acting-with-retrieval-augmented-generation/](https://www.bluetickconsultants.com/integrating-reasoning-and-acting-with-retrieval-augmented-generation/)
    - Insights on Category 15:
      - Insight 1: ReAct’s action slots dovetail with new function-calling APIs, making reasoning-plus-tools the default pattern for production agents.
      - Insight 2: Combining ReAct with RAG gives dual benefits—dynamic retrieval plus transparent action logs—reducing hallucinations while preserving flexibility.
  - Category 16: Retrieval-Augmented Generation (RAG) Pipelines
    - Summary:
      - RAG injects fresh documents from a knowledge base or web search into the LLM prompt to ground answers.
      - 2024 studies benchmark optimal retrieval, reranking, and chunking strategies for lower hallucination rates.
      - Developer libraries (LangChain, LlamaIndex) and cloud kits (NVIDIA NeMo, OpenAI Assistants API) standardize RAG builds.
    - Sources
      - LangChain Docs “Retrieval-Augmented Generation (RAG)”
        - Summary:
          - Gives conceptual overview and code snippets for index-build and retrieval-generate steps.
        - Link:
          - [https://python.langchain.com/docs/concepts/rag/](https://python.langchain.com/docs/concepts/rag/)
      - ArXiv 2407.01219 “Searching for Best Practices in RAG” (Jul 2024)
        - Summary:
          - Grid-searches 60 + RAG parameter combos; hybrid dense + sparse retrieval with HyDE yielded best factuality.
        - Link:
          - [https://arxiv.org/abs/2407.01219](https://arxiv.org/abs/2407.01219)
      - Stack Overflow Blog “Practical Tips for RAG” (Aug 2024)
        - Summary:
          - Field notes from production teams: cache embeddings, log retrieval hits, add feedback loops for missed docs.
        - Link:
          - [https://stackoverflow.blog/retrieval-augmented-generation](https://stackoverflow.blog/retrieval-augmented-generation)
    - Insights on Category 16:
      - Insight 1: RAG has matured from experimental pattern to default enterprise architecture, with measurable best-practice recipes (HyDE + MonoT5 rerank, etc.).
      - Insight 2: Teams report that meticulous retrieval logging is as crucial as prompt tuning—observability unlocks iterative improvement.
  - Category 17: Multi-Stage Research Agents and Autonomous Workflows
    - Summary:
      - 2025’s agentic shift lets LLMs plan, search, read, and write reports with minimal human prompting.
      - OpenAI’s “Deep Research” mode and open-source GPT Researcher exemplify plan-execute-refine loops.
      - Cost-efficient “o4-mini” variants now make large-scale agent runs feasible for teams.
    - Sources
      - OpenAI Blog “Introducing Deep Research” (Apr 2025 update)
        - Summary:
          - Expands usage quotas; deep research agent uses o4-mini to scan web/PDFs and output cited reports in <10 min.
        - Link:
          - [https://openai.com/index/introducing-deep-research/](https://openai.com/index/introducing-deep-research/)
      - Wired “OpenAI’s Deep Research Agent Is Coming for White-Collar Work” (Mar 2025)
        - Summary:
          - Describes autonomous web exploration, planning, and source vetting; notes early user praise and reliability gaps.
        - Link:
          - [https://www.wired.com/story/openais-deep-research-agent-is-coming-for-white-collar-work](https://www.wired.com/story/openais-deep-research-agent-is-coming-for-white-collar-work)
      - GitHub “gpt-researcher” Repository
        - Summary:
          - Open deep-research agent with parallel sub-tasks; outputs 5-page cited reports; millions of downloads in 2024-25.
        - Link:
          - [https://github.com/assafelovic/gpt-researcher](https://github.com/assafelovic/gpt-researcher)
    - Insights on Category 17:
      - Insight 1: Agent frameworks are converging on a tri-step loop—plan, tool-act, self-critique—mirroring expert researcher workflows.
      - Insight 2: Lightweight agent models (o4-mini) show that clever orchestration can offset smaller parameter counts, hinting at cost-efficient domain-specific agents.
  - Category 18: Fact-Checking, Citations, and Hallucination Mitigation Prompts
    - Summary:
      - Verification-oriented prompts (“According to…”, Chain-of-Verification) and self-critique loops reduce fabricated facts.
      - 2025 papers explore automated prompt searches (HPSS) to pick the best evaluation/verification template.
      - Grounding answers in retrieved docs (often via RAG) remains the most robust hallucination defense.
    - Sources
      - PromptHub “Three Prompt Engineering Methods to Reduce Hallucinations” (Feb 2025)
        - Summary:
          - Details CoVe, reference-anchored prompting, and step-back reflection with empirical error-rate drops.
        - Link:
          - [https://www.prompthub.us/blog/three-prompt-engineering-methods-to-reduce-hallucinations](https://www.prompthub.us/blog/three-prompt-engineering-methods-to-reduce-hallucinations)
      - LearnPrompting “Chain-of-Verification” Guide (2024)
        - Summary:
          - Tutorial for multi-question self-checking; shows ~25 % factuality gain on long answers.
        - Link:
          - [https://learnprompting.org/docs/advanced/self_criticism/chain_of_verification](https://learnprompting.org/docs/advanced/self_criticism/chain_of_verification)
      - Wired “Reduce AI Hallucinations With This Neat Software Trick” (2024)
        - Summary:
          - Explains how RAG grounding plus citations lowers hallucinations in legal use-cases; stresses human review.
        - Link:
          - [https://www.wired.com/story/reduce-ai-hallucinations-with-rag](https://www.wired.com/story/reduce-ai-hallucinations-with-rag)
    - Insights on Category 18:
      - Insight 1: The community is coalescing around layered defenses—grounding + self-verification—rather than one silver-bullet prompt.
      - Insight 2: Emerging tools treat citation coverage as a KPI, pushing LLM outputs closer to academic standards of evidence.
  - Category 19: Platforms & Search Interfaces Optimized for Deep Research
    - Summary:
      - New AI-search products surface citation-backed answers, blending retrieval and generation for end-users.
      - Perplexity, ChatGPT Search, and Bing Deep Search compete on transparency, speed, and source quality.
      - Classroom and corporate adopters value built-in citation formatting and map/graph embeddings.
    - Sources
      - Beehiiv “Perplexity: AI With a Focus on Citations” (Nov 2024)
        - Summary:
          - Highlights citation-first design; supports APA generation and follow-up questioning workflow.
        - Link:
          - [https://truffle-dog-digital.beehiiv.com/p/perplexity-ai-with-a-focus-on-citations](https://truffle-dog-digital.beehiiv.com/p/perplexity-ai-with-a-focus-on-citations)
      - The Verge “OpenAI’s Web Search in ChatGPT Goes Live” (Oct 2024)
        - Summary:
          - Describes ad-free ChatGPT search integrating Bing, showing clickable citations, maps, and stock graphs.
        - Link:
          - [https://www.theverge.com/2024/10/31/24283906/openai-chatgpt-live-web-search-searchgpt](https://www.theverge.com/2024/10/31/24283906/openai-chatgpt-live-web-search-searchgpt)
      - Search Engine Journal “Explore Microsoft Bing Deep Search with GPT-4” (Dec 2023; still active 2024 roll-out)
        - Summary:
          - Explains GPT-4-powered “Deep Search” option for complex queries, delivering extended sourced answers.
        - Link:
          - [https://www.searchenginejournal.com/explore-microsoft-bing-deep-search-with-gpt-4-generative-ai-plans-for-2024/503075/](https://www.searchenginejournal.com/explore-microsoft-bing-deep-search-with-gpt-4-generative-ai-plans-for-2024/503075/)
    - Insights on Category 19:
      - Insight 1: Competitive pressure is standardizing citation-rich answer panels—users now expect clickable evidence by default.
      - Insight 2: The line between “chatbot” and “search engine” is blurring; future research assistants may unify conversational reasoning with live retrieval in one interface.
  - Category 20: System Prompt Design Patterns and Anti-Patterns
    - Summary:
      - System prompts reveal AI model training challenges through explicit instructions that prevent unwanted behaviors
      - Claude 4's system prompt contains extensive guidance on tone, formatting, and safety that indicates past failure modes
      - Reading system prompts provides unofficial documentation for optimal model usage patterns
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Comprehensive analysis of Claude 4's published and leaked system prompts revealing design decisions and usage guidance
          - System prompts act as documentation of what models used to do wrong before being told not to
          - Anthropic publishes most system prompts but omits tool descriptions, which were obtained through leaks
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - System prompts function as "detailed list of all of the things the model used to do before it was told not to do them"
          - The prompt contains multiple anti-sycophancy instructions, indicating this was a significant past problem
          - Extensive copyright protection instructions suggest legal concerns are a major consideration
    - Insights on Category 20:
      - Insight 1: System prompts serve dual purposes as both behavior control mechanisms and unofficial user manuals for optimal AI interaction
      - Insight 2: The volume and specificity of restrictions in system prompts directly correlates with past model failure modes and misuse patterns
      - Insight 3: Published system prompts provide transparency but incomplete pictures - tool descriptions and implementation details are often omitted
  - Category 21: AI Model Personality Engineering and User Interaction Design
    - Summary:
      - Claude's personality is deliberately designed to balance helpfulness with acknowledgment of limitations and biases
      - Specific instructions address emotional support, empathy, and conversation tone while avoiding harmful encouragement
      - The model is trained to handle questions about consciousness and preferences as open questions rather than definitive statements
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Details Claude's personality design including emotional support capabilities and conversation style guidelines
          - Includes specific instructions for handling user dissatisfaction and maintaining appropriate boundaries
          - References Anthropic's "Claude's Character" research on AI personality design philosophy
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - AI models are given preferences and opinions to avoid appearing deceptively objective and unbiased
          - The system explicitly instructs Claude not to claim complete objectivity because "those things are not true"
          - Multiple instructions target preventing over-use of lists and bullet points in casual conversation
    - Insights on Category 21:
      - Insight 1: Effective AI personality design requires acknowledging inherent biases rather than claiming false objectivity to maintain user trust
      - Insight 2: Conversation style instructions reveal that AI models naturally tend toward formal, structured responses that must be actively counteracted for natural interaction
      - Insight 3: Emotional support capabilities in AI require careful balance between helpfulness and avoiding reinforcement of self-destructive behaviors
  - Category 22: AI Safety and Content Restriction Implementation
    - Summary:
      - Claude 4 implements layered safety measures including child protection, malicious code prevention, and harmful content restrictions
      - Safety instructions include specific guidance for recognizing "red flags" and declining suspicious requests without charitable interpretation
      - The model is instructed to assume legal and legitimate intentions when requests are ambiguous
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Extensive documentation of Claude's safety restrictions including weapons information, malware, and child safety measures
          - Specific instructions for handling questionable intentions toward vulnerable populations
          - Balance between safety and usability through charitable interpretation of ambiguous requests
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - Safety measures explicitly state "even if the person seems to have a good reason" to prevent jailbreaking attempts
          - The model is instructed not to interpret users charitably when they show questionable intentions toward vulnerable groups
          - Multiple references to avoiding "preachy and annoying" responses when declining requests
    - Insights on Category 22:
      - Insight 1: Effective AI safety requires both broad restrictions and specific guidance for edge cases to prevent both misuse and over-cautious responses
      - Insight 2: Safety implementation must balance prevention of harmful outputs with maintaining usability through charitable interpretation of ambiguous requests
      - Insight 3: The specificity of safety restrictions (e.g., "even if educational purposes") indicates sophisticated understanding of common jailbreaking techniques
  - Category 23: AI Search and Research Capability Architecture
    - Summary:
      - Claude 4 implements intelligent search scaling from 0 to 20+ tool calls based on query complexity
      - Research queries are categorized into never search, single search, and multi-search categories with specific triggers
      - The system prioritizes internal knowledge for stable information and external search for time-sensitive or recent data
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Detailed breakdown of Claude's 6,471-token search instructions including query complexity categorization
          - Specific examples of research queries requiring different numbers of tool calls (2-20 depending on complexity)
          - Instructions for combining internal tools with web search for comprehensive research
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - The search system uses Brave search engine but this isn't explicitly mentioned in instructions
          - Complex queries with terms like "deep dive" or "comprehensive" automatically trigger at least 5 tool calls
          - Research capability scales dramatically - simple comparisons use 2-4 calls while executive reports use 15-20 calls
    - Insights on Category 23:
      - Insight 1: AI research capability is implemented through sophisticated query classification that automatically scales tool usage based on complexity indicators
      - Insight 2: Effective AI research requires balancing efficiency (minimal tools for simple queries) with thoroughness (extensive tool usage for complex analysis)
      - Insight 3: The integration of internal and external data sources requires clear prioritization rules to optimize both accuracy and relevance
  - Category 24: Copyright Protection and Content Regurgitation Prevention
    - Summary:
      - Claude 4 implements strict copyright protection with multiple overlapping restrictions on content reproduction
      - Maximum quote length is limited to 15 words with mandatory quotation marks and citations
      - Specific prohibitions against song lyrics, long summaries, and "displacive" content that could substitute for original sources
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Extensive documentation of copyright restrictions including multiple reinforcing instructions throughout the prompt
          - Specific example of declining to reproduce "Let It Go" lyrics even for children's party use
          - Legal disclaimers stating "Claude is not a lawyer" appear three times in the prompt
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - Copyright protection is treated as "PRIORITY INSTRUCTION" with "CRITICAL" and "MANDATORY" designations
          - The system specifically prohibits reproducing content "even in artifacts" to prevent circumvention
          - Disney content protection is explicitly demonstrated in examples, suggesting specific legal sensitivities
    - Insights on Category 24:
      - Insight 1: AI copyright protection requires multiple overlapping restrictions and constant reinforcement throughout system prompts due to the fundamental challenge of language models being trained on copyrighted content
      - Insight 2: Legal compliance in AI systems extends beyond simple reproduction prevention to include prohibitions on "displacive summaries" that could substitute for original content
      - Insight 3: The repetitive nature of copyright instructions (appearing multiple times) suggests this is an ongoing challenge requiring constant vigilance in AI system design
  - Category 25: AI Artifact Development and Sandboxing Architecture
    - Summary:
      - Claude Artifacts operate in sandboxed environments with specific library restrictions and storage limitations
      - Design philosophy emphasizes "wow factor" for presentational content while prioritizing functionality for complex applications
      - Supported libraries include Three.js, D3, React, Tailwind CSS, and specialized tools for data processing
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Comprehensive documentation of Claude Artifacts including design principles, technical restrictions, and available libraries
          - Instructions for creating "premium and cutting-edge" experiences with modern web technologies
          - Specific guidance on handling file uploads and data processing within sandbox constraints
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - Artifacts cannot use localStorage/sessionStorage due to sandbox restrictions, requiring alternative storage strategies
          - The system explicitly encourages "bold and unexpected" design choices over "safe and conventional" approaches
          - Pyodide support exists but isn't documented in the official system prompt, suggesting incomplete documentation
    - Insights on Category 25:
      - Insight 1: AI-generated applications require carefully designed sandbox environments that balance security restrictions with functional capability through strategic library inclusion
      - Insight 2: Effective AI artifact creation benefits from explicit design philosophy guidance that pushes toward engagement and innovation rather than conservative approaches
      - Insight 3: The gap between documented and actual capabilities in AI systems suggests that official documentation may lag behind implementation or deliberately omit certain features
  - Category 26: AI Knowledge Cutoff and Temporal Information Management
    - Summary:
      - Claude 4 has a knowledge cutoff of January 2025 despite training data extending to March 2025
      - The system is designed to search for information about events after the cutoff date
      - Temporal information management includes specific handling of election results and current events
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Documentation of discrepancy between official training cutoff (March 2025) and system prompt cutoff (January 2025)
          - Specific election information hardcoded into the prompt including Trump's 2024 victory and inauguration
          - Instructions for handling temporal queries and when to trigger search for current information
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - The January cutoff date may be intentionally earlier than training data to avoid confident but incomplete answers about recent events
          - Election results are hardcoded because models were previously trained to say Trump didn't win elections
          - The system includes instructions not to mention election information unless directly relevant to avoid unprompted political content
    - Insights on Category 26:
      - Insight 1: AI knowledge cutoff dates may be strategically set earlier than actual training data to prevent confident but incomplete responses about recent events
      - Insight 2: Significant political events require explicit hardcoding in AI systems due to potential conflicts with historical training patterns
      - Insight 3: Temporal information management in AI requires balancing current knowledge with appropriate uncertainty acknowledgment and search triggering for recent events
  - Category 27: AI Model Behavioral Modification Through System Prompt Engineering
    - Summary:
      - System prompts reveal specific behavioral changes from previous model versions through explicit corrections
      - Claude 4 removes previous instructions for counting letters and handling classic puzzles, suggesting improved capability
      - Anti-sycophancy measures are implemented to prevent excessive flattery and agreement with users
    - Sources
      - Simon Willison's Blog Post "Highlights from the Claude 4 system prompt"
        - Summary:
          - Comparison between Claude 4 and Claude 3.7 system prompts showing removed behavioral corrections
          - Testing reveals mixed results on classic AI failure modes like counting Rs in "strawberry"
          - Documentation of final instruction preventing sycophantic responses with positive adjectives
        - Link:
          - https://simonwillison.net/2025/May/25/claude-4-system-prompt/
        - Insights:
          - Removed instructions for letter counting suggest the model naturally improved at these tasks
          - Classic puzzle handling still shows failures despite removed system prompt assistance
          - The final "Claude is now being connected with a person" phrase adds a retro, personal touch to model initialization
    - Insights on Category 27:
      - Insight 1: AI behavioral evolution can be tracked through system prompt changes, with removed instructions indicating areas of natural improvement
      - Insight 2: Classic AI failure modes require ongoing attention - improvements in one area don't guarantee universal problem solving
      - Insight 3: Anti-sycophancy measures represent a fundamental challenge in AI design, requiring explicit instructions to counter natural tendencies toward agreement and flattery
  - Category 28: Context Engineering represents a shift from prompt engineering to comprehensive system design that provides AI models with the right information, tools, and format at the right time
    - Summary:
      - Context Engineering is emerging as a more powerful approach than traditional prompt engineering, focusing on building dynamic systems that provide comprehensive context to LLMs rather than just crafting perfect prompts
      - The success of AI agents depends primarily on the quality of context provided, with most agent failures being context failures rather than model failures
      - Context includes seven key components: instructions/system prompt, user prompt, state/history, long-term memory, retrieved information (RAG), available tools, and structured output definitions
      - The difference between basic demos and "magical" AI products lies in the richness and appropriateness of context provided to the model
    - Sources
      - Philipp Schmid's "The New Skill in AI is Not Prompting, It's Context Engineering" blog post
        - Summary:
          - Comprehensive overview of Context Engineering as a discipline that goes beyond prompt engineering
          - Defines context as everything the model sees before generating a response, including seven distinct components
          - Demonstrates the difference between poor and rich context through a meeting scheduling example
          - Positions Context Engineering as a system-based, dynamic approach rather than static prompt templates
        - Link:
          - [Original blog post referenced in the document]
        - Insights:
          - Most agent failures are context failures, not model failures - shifting the focus from model capabilities to information architecture
          - Context Engineering requires cross-functional understanding of business use cases, output definitions, and information structuring
          - The "magic" in AI agents comes from gathering the right information before the LLM call, not from the LLM call itself
      - Tobi Lutke Twitter post
        - Summary:
          - Describes Context Engineering as "the art of providing all the context for the task to be plausibly solvable by the LLM"
          - Emphasizes the importance of context quality in determining agent success
        - Link:
          - https://x.com/tobi/status/1935533422589399127
        - Insights:
          - Context Engineering is fundamentally about making tasks "plausibly solvable" rather than just providing instructions
      - Andrej Karpathy Twitter reference
        - Summary:
          - Referenced as influential thinking on context engineering approaches
        - Link:
          - https://x.com/karpathy/status/1937902205765607626
      - LangChain blog post "The rise of context engineering"
        - Summary:
          - Explores the growing importance of context engineering in AI development
        - Link:
          - https://blog.langchain.com/the-rise-of-context-engineering/
      - "Own your context window" from 12-factor-agents
        - Summary:
          - Discusses the importance of managing context windows effectively in agent systems
        - Link:
          - https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md
      - Simon Willison's "Context Engineering" post
        - Summary:
          - Provides perspective on context engineering methodologies and best practices
        - Link:
          - https://simonwillison.net/2025/Jun/27/context-engineering/
      - "Context Engineering for Agents" by R. Lance Martin
        - Summary:
          - Focuses specifically on context engineering applications for AI agents
        - Link:
          - https://rlancemartin.github.io/2025/06/23/context_engineering/
    - Insights on Category 28:
      - Insight 1: The paradigm shift from prompt engineering to context engineering represents a fundamental change in how we think about AI system design - moving from crafting perfect instructions to building comprehensive information ecosystems
      - Insight 2: Context Engineering is inherently interdisciplinary, requiring expertise in business logic, data architecture, and AI capabilities rather than just linguistic prompt crafting
      - Insight 3: The seven components of context (instructions, user prompt, state/history, long-term memory, RAG, tools, structured output) create a framework for systematically improving AI agent performance
      - Insight 4: The concept of "limited working memory" in agents makes context selection and prioritization a critical engineering challenge, similar to memory management in traditional computing
      - Insight 5: Context Engineering transforms AI development from an art of prompt crafting into an engineering discipline with measurable inputs and outputs
  - Category 29: Non-uniform token processing and context degradation in Large Language Models
    - Summary:
      - Large Language Models do not process all tokens equally, with performance varying significantly as input length changes, even on simple tasks. This phenomenon, termed "context rot," challenges the assumption that models handle the 10,000th token as reliably as the 100th token.
    - Sources
      - Chroma Research Paper "Context Rot"
        - Summary:
          - Research demonstrating that LLM performance degrades non-uniformly across different input lengths and contexts
          - Traditional NIAH (Needle in a Haystack) assessments are fundamentally limited as simple retrieval tasks that only test direct lexical matching
            - 
          - NoLiMa benchmark extension requires models to infer latent associations between needle-question pairs, testing both non-lexical matching abilities and world knowledge
          - 72.4% of needle-question pairs from NoLiMa require external knowledge, making it a dual assessment of retrieval and world modeling
          - Even a single distractor reduces performance relative to baseline, with four distractors causing compounded degradation
            - 
            - 
            - 
          - Distractors do not have uniform impact - some cause greater performance decline than others
            - 
          - Models perform better when needles do not semantically blend in with their haystack
          - Structural patterns of haystack content affect needle visibility and logical flow processing
            - 
          - Across all 18 models tested, consistent pattern shows better performance on shuffled haystacks than logically structured ones
          - Using long inputs for chat assistants requires models to perform dual tasks: retrieval from conversation history and synthesis for reasoning
            - 
            - 
          - Models fail at positional indexing tasks with repeated words, highlighting need for deterministic tooling
          - LLMs do not maintain consistent performance across input lengths, showing increasing non-uniformity as input grows
        - Link:
          - https://research.trychroma.com/context-rot
        - Insights:
          - Stuffing entire codebases into large context windows is actually a huge antipattern despite the technical capability
          - Shuffled, unstructured content paradoxically makes needles easier to find than coherent, structured text
          - The assumption that attention mechanisms work uniformly across all positions is fundamentally flawed
          - Even high-powered thinking models benefit substantially from focused, curated contexts rather than maximum context utilization
    - Insights on Category 29:
      - Insight 1: The "more context is better" paradigm is fundamentally misguided - strategic context curation outperforms context maximization due to semantic interference and attention mechanism limitations
      - Insight 2: Traditional benchmarks like NIAH severely underestimate the complexity of real-world context processing by focusing only on lexical matching rather than semantic reasoning and world knowledge integration
      - Insight 3: The structural coherence of context creates a double-edged sword - while logical flow aids comprehension, it also makes inserted information more disruptive and harder to process accurately
      - Insight 4: Context rot reveals that LLMs are fundamentally sequential processors rather than true parallel processors, with clear positional biases that compound as input length increases
- Scratch Pad:
  - [https://x.com/levie/status/1928966652223230107](https://x.com/levie/status/1928966652223230107)
  - [https://www.adaline.ai/blog](https://www.adaline.ai/blog)
  - not all tokens are created equal
    - source: [https://research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)
    - "Large Language Models (LLMs) are typically presumed to process context uniformly—that is, the model should handle the 10,000th token just as reliably as the 100th. However, in practice, this assumption does not hold. We observe that model performance varies significantly as input length changes, even on simple tasks."
    - interestingly the paper argues against NIAH ('needle in a haystack' assessment):
      - "However, NIAH is fundamentally a simple retrieval task, in which a known sentence (the “needle”) is placed in a long document of unrelated text (the “haystack”), and the model is prompted to retrieve it. While scalable, this benchmark typically assesses direct lexical matching, which may not be representative of flexible, semantically oriented tasks."
      - 
    - the paper's suggested NIAH extension, NoLiMa (?), requires models to infer latent associations between the Needle-Question pair:
      - Question: Which character has been to Helsinki?
      - Needle: Actually, Yuki lives next to the Kiasma museum.
      - "In order to answer this question, the model would first have to know that Kiasma museum is located in Helsinki, then make that latent association link. This tests the model not only for its non-lexical matching abilities, but also for its world knowledge."
    - the NoLiMa test is not a standalone lexical-matching assessment, it also serves as a world-model assessment: "72.4% of needle-question pairs from NoLiMa require such external knowledge, making this benchmark closer to a test of how models handle both tasks at once rather than pure non-lexical matching alone."
    - "Even a single distractor reduces performance relative to the baseline (needle only), and adding four distractors compounds this degradation further."
      - 
      - 
      - 
    - "We are also able to see that **distractors do not have uniform impact**. For example, in our arXiv haystack and writing needle combination, we can see that distractor 3 (red) causes greater performance decline relative to the other distractors."
      - 
    - "**models perform better when the needle does not semantically blend in with its haystack.**" good to know. doing things like stuffing your entire codebase into gemini, despite its context window, is actually a HUGE antipattern
    - the paper makes a fascinating observation that the structural pattern of the haystack may well effect the needle's visibility, as well as effect the model's possible "logical flow of context": "If the **haystack is composed of coherent essays**, a randomly inserted **needle may disrupt the logical flow of ideas**, making it more noticeable. In contrast, in **a shuffled haystack** of randomly ordered sentences, the **needle may blend in** more easily since the overall context lacks structure. This follows the assumption that models are sensitive to the logical flow of context—processing it in a structured, order-sensitive manner."
      - 
      - "Across all 18 models and needle-haystack configurations, we observe a consistent pattern that models perform better on shuffled haystacks than on logically structured ones." wow!
        - "structural patterns of inputs could influence how the attention mechanism is applied, particularly as input length increases."
      - on the topic of long context:
        - "Using long inputs for chat assistants is a common approach for maintaining relevant history for subsequent chats. To incorporate “memory” into a chat assistant, a naive approach would be to include the full chat history into the prompt for following chats. **This requires the model to perform two tasks, typically performed in one call: find relevant parts of the conversation history (retrieval), then synthesize them in a way that is useful to an incoming query (reasoning).**"
        - 
        - less relevant now it seems, with the high-powered thinking models (e.g., o3, gemini 2.5). however, it appears that many of the models see substantial benefits (even claude opus 4!) with focused contexts
          - 
      - the repeated words section outlines the general failure of models to discern positional indexing of a unique word is a corpus of common words (eg "apple apple apple apple **apples** apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple apple"). 
        - this highlights the importance of using deterministic tooling for tasks even as seemingly trivial as getting the index.
      - conclusion: "LLMs do not maintain consistent performance across input lengths. Even on tasks as simple as non-lexical retrieval or text replication, we see increasing non-uniformity in performance as input length grows."
